リランカーを使用した 2 段階検索の実装
2 段階の取得とリランカーの背後にある原理を理解したところで、RAG システムのコンテキスト内での実際の実装を検討してみましょう。一般的なライブラリとフレームワークを活用して、これらの手法の統合を実証します。

環境の設定
コードに入る前に、開発環境をセットアップしましょう。 Python と、Hugging Face Transformers、Sentence Transformers、LanceDB などのいくつかの人気のある NLP ライブラリを使用します。

1
2
# Install required libraries
!pip install datasets huggingface_hub sentence_transformers lancedb
データの準備
デモンストレーションの目的で、Hugging Face Datasets の「ai-arxiv-chunked」データセットを使用します。このデータセットには、機械学習、自然言語処理、大規模言語モデルに関する 400 以上の ArXiv 論文が含まれています。

1
2
3
from datasets import load_dataset
dataset = load_dataset("jamescalam/ai-arxiv-chunked", split="train")
<pre>
次に、データを前処理し、より小さなチャンクに分割して、効率的な取得と処理を容易にします。

1
2
3
4
5
6
7
8
9
10
11
12
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
def chunk_text(text, chunk_size=512, overlap=64):
tokens = tokenizer.encode(text, return_tensors="pt", truncation=True)
chunks = tokens.split(chunk_size - overlap)
texts = [tokenizer.decode(chunk) for chunk in chunks]
return texts
chunked_data = []
for doc in dataset:
text = doc["chunk"]
chunked_texts = chunk_text(text)
chunked_data.extend(chunked_texts)
最初の検索段階では、Sentence Transformer モデルを使用してドキュメントとクエリを高密度ベクトル表現にエンコードし、LanceDB のようなベクトル データベースを使用して近似最近傍検索を実行します。
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
from sentence_transformers import SentenceTransformer
from lancedb import lancedb
# Load Sentence Transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')
# Create LanceDB vector store
db = lancedb.lancedb('/path/to/store')
db.create_collection('docs', vector_dimension=model.get_sentence_embedding_dimension())
# Index documents
for text in chunked_data:
vector = model.encode(text).tolist()
db.insert_document('docs', vector, text)
from sentence_transformers import SentenceTransformer
from lancedb import lancedb
# Load Sentence Transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')
# Create LanceDB vector store
db = lancedb.lancedb('/path/to/store')
db.create_collection('docs', vector_dimension=model.get_sentence_embedding_dimension())
# Index documents
for text in chunked_data:
vector = model.encode(text).tolist()
db.insert_document('docs', vector, text)
ドキュメントにインデックスが付けられているので、特定のクエリ ベクトルに最も近いものを見つけることで初期検索を実行できます。

1
2
3
4
5
6
7
8
9
10
11
12
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
def chunk_text(text, chunk_size=512, overlap=64):
tokens = tokenizer.encode(text, return_tensors="pt", truncation=True)
chunks = tokens.split(chunk_size - overlap)
texts = [tokenizer.decode(chunk) for chunk in chunks]
return texts
chunked_data = []
for doc in dataset:
text = doc["chunk"]
chunked_texts = chunk_text(text)
chunked_data.extend(chunked_texts)
再ランキング
最初の取得後、再ランキング モデルを使用して、クエリとの関連性に基づいて、取得したドキュメントを並べ替えます。この例では、ドキュメントのランキング用に特別に設計された高速で正確なトランスフォーマー ベースのモデルである ColBERT リランカーを使用します。

1
2
3
4
from lancedb.rerankers import ColbertReranker
reranker = ColbertReranker()
# Rerank initial documents
reranked_docs = reranker.rerank(query, initial_docs)
　 reranked_docs リストには、ColBERT リランカーによって決定された、クエリとの関連性に基づいて並べ替えられたドキュメントが含まれるようになりました。

拡張と生成
再ランク付けされた関連ドキュメントが手元にあれば、RAG パイプラインの拡張および生成段階に進むことができます。 Hugging Face Transformers ライブラリの言語モデルを使用して、最終応答を生成します。

1
2
3
4
5
6
7
8
9
10
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
# Augment query with reranked documents
augmented_query = query + " " + " ".join(reranked_docs[:3])
# Generate response from language model
input_ids = tokenizer.encode(augmented_query, return_tensors="pt")
output_ids = model.generate(input_ids, max_length=500)
response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(response)
