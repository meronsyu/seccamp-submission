{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meronsyu/seccamp-submission/blob/main/multiEpochs500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "7817fb6e-e56a-467b-be73-36110d4ba79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-18 05:05:47--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-05-18 05:05:48 (19.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomize the order\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 500\n",
        "num_epoches = 10\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# make same_dataset\n",
        "same_dataset = []\n",
        "for iter in range(max_iters):\n",
        "  xb, yb = get_batch('train')\n",
        "  same_dataset.append((xb, yb))\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epoches):\n",
        "  # Randomize the order\n",
        "  random.shuffle(same_dataset)\n",
        "  for iter in range(max_iters):\n",
        "\n",
        "      # sample a batch of data\n",
        "      xb, yb= same_dataset[iter]\n",
        "\n",
        "      # evaluate the loss\n",
        "      logits, loss = model(xb, yb)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  # generate from the model\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  losses = estimate_loss()\n",
        "  print(f\"train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  print(f\"Epoch {epoch + 1} generated text:\")\n",
        "  print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "48717027-4443-4afe-f839-f7e084ebcf7a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "train loss 2.2957, val loss 2.3052\n",
            "Epoch 1 generated text:\n",
            "\n",
            "CANCIODkRO:\n",
            "Thowile: O lay bermad seak obe don.\n",
            "SagO-d my:\n",
            "Cachands:\n",
            "Wanthar ulqur hent? hedXlas ate awice my.\n",
            "\n",
            "HDER:\n",
            "AnzowN he owns, tof isth ble mil nowlll,\n",
            "Wh iree senghin lat Het drov tha dof Win nour ilerjes!\n",
            " lolind me litenser onchiry: thar aiss hew ye whe mal nor\n",
            "Touteld desthethy wod mothakllo Wind! whe Ceiibke the mof man, The hige he poof ower; thure kad nonrtht f sor; irker my mour yofler,\n",
            "Thefaf Pre? gron? INr fursea!\n",
            "\n",
            "Wid &is:\n",
            "Sadsal the E's sthidin couk ay andy Iry thachan thove you meand, bem gy.\n",
            "You Inot wheam sor tow hin.-\n",
            "\n",
            "KINNOR:\n",
            "Nome mreand tho mablin soth lough ome.\n",
            "\n",
            "Thuch ffepy thathth sou.\n",
            " AnEnds dethal wave.\n",
            "se eandew he movet: imd asst to os if mee thatie so nou heare feRil as ny, cou longhasknd\n",
            "Whown: du heve, moxle achethe beaker aghercebun'ss m sou wif luml nem,\n",
            "Whor wllo no thive, mche hI surd?\n",
            "Theid hith bron'end, wie? Iy\n",
            "Ke thistou tiue the onof the fut no aporth ou whand nod thacou\n",
            "Ast, Blowllke to thathe\n",
            "Fo pan, de wat do ive wout ir fof Gly-eiche oue\n",
            "train loss 2.1017, val loss 2.1295\n",
            "Epoch 2 generated text:\n",
            "\n",
            "MARINGLAM:\n",
            "A for farickeQt Henk;\n",
            "Whuch Rutck it the priveisanoun,\n",
            "On bay hour have men, mese things so le wortingin ith to highe Wein word,\n",
            "Dodce now brane: hichenIn ast a theing do day, the and inke it blave at my brend eire brwmeng to shave e would is thave thouy's her weertmpermlelslourg the of thee drjeaked fran,\n",
            "Sor umth Of haver masting it ily.\n",
            "\n",
            "LARGKICHQT:\n",
            "Hound the now, muths bend sey o nillb,\n",
            "On I y gaikl vit bell hor make him nong to me with te your.\n",
            "\n",
            "LUCHAUSTAPLA:\n",
            "He,\n",
            "BURO:\n",
            "BRIOMER:\n",
            "\n",
            "Thins gooookest but hap-der, sages soun her thouk\n",
            "-fat cace:\n",
            "On! proue of wher sefll whrist ard geven,\n",
            "The with ham kLer brathe hiph it\n",
            "'lorrd shem mroud u arw'll-Gark.\n",
            "\n",
            "Sive af ben of mhirsinsorss o'd;\n",
            "\n",
            "Thith sI have blay noteld, thir flouth shim I I heach courcist, of monge,\n",
            "Aodlf tall we twith se tonf it a se of tre bus the peeroper lik;\n",
            "That o'lf gungo do shee, of mack goo.\n",
            "\n",
            "CuF woury promblak now wonse troJea.\n",
            "\n",
            "What ED VINCENTIUS:\n",
            "Hoat in tour is shoun's for athme stemerour heh nide a \n",
            "Mare\n",
            "train loss 1.9886, val loss 2.0545\n",
            "Epoch 3 generated text:\n",
            "\n",
            "B; enlant, liver, his lome ay bet you colgron.\n",
            "Grad high merasst youre, Aid the eneerd\n",
            "Burt thur done is or kie well the lale foo his enverd am not bsorme word\n",
            "O, itis it holde\n",
            "Thee hein buss you ptward is corser weand\n",
            "hre me that ne there lamor, here\n",
            "Podshencend donht lamman:\n",
            "As go your as is is coudrien me whers;\n",
            "Sherap tou of ill.\n",
            "\n",
            "DUKIUSClicelome, and be mar my prent grast. Sell me.\n",
            "\n",
            "DUKEN ELINVEO:\n",
            "I'lee, one shou maroll cinnot this where\n",
            "Iid the your jepred wills for ours.\n",
            "\n",
            "ROMIqut crand lemfine.\n",
            "\n",
            "ICDINARDY IOLA:\n",
            "Viseend to go me mear carvaingloake.\n",
            "Aads have ar enver on where hawind set!\n",
            "Thou' He whand, the: ponire thank be or more.\n",
            "\n",
            "Merry Io would the fichore pasty me werreing onecloin? I nearse than his yrequ, have you roin kirss wich and fime thoun it.\n",
            "\n",
            "Cearss, Lere buth gharso mattant woand.\n",
            "\n",
            "RORBROUCHAE:\n",
            "\n",
            "VIAll the prace isher, and Done\n",
            "the prome in carnt. Yet you him his hast, of that he dencer his his ber himsen?\n",
            "To kthey to you my yhave he jrad bre'e,\n",
            "Is I ilf you ha my t\n",
            "train loss 1.9029, val loss 1.9975\n",
            "Epoch 4 generated text:\n",
            "\n",
            "Shan chale wem hick statitews, my father,\n",
            "I kquink I neg, a inclous thim all their sore:\n",
            "Lets and thy are willard; I it sto ort?--\n",
            "\n",
            "HERRUTIN:\n",
            "I playseiner a tick'd ahter one your as mone ormel's,\n",
            "And your all know, greckem, with The yours a prose hing on,\n",
            "hand hus ance on, my sseand, but fathir thze not to sprears. Thy mever it bose callothel.\n",
            "When at enrouse them owa will to mese, and facharch?old He canstoord, ap ane your and youar of applegerd the carce a mxplitining then?\n",
            "Sarn shond maise buly neplo.\n",
            "Thealkionn: foe, or ary benss uncheged a devess agenslain you,\n",
            "Ither sae man peastles and bowars my lay-bretervet greats, this bestluind in an my as knee;\n",
            "And lequb and retarespoub'd she wee hads not\n",
            "Of if the bAy asseen yecunity hold strone of Onge courss and teecee?\n",
            "And tisbind recome his\n",
            "A thiqustim instenfy him ca!\n",
            "Hers and usto surter or you cont;\n",
            "Your dighiss selamet enewer-sty,\n",
            "There the freath's bes, ancion, I the liquesie ofurased-black? An of my nagenmen\n",
            "Take own; she cally w\n",
            "train loss 1.8501, val loss 1.9666\n",
            "Epoch 5 generated text:\n",
            "\n",
            "GLORYCUTIO:\n",
            "Lord, go shall may coman's you the delliny, I leke of my cowsve?\n",
            "\n",
            "SARIANTET:\n",
            "I'ly so swand Rome it the starkly screfor, son tho is All thee then, you bey be moth are hosp gaid: tume diss had?\n",
            "\n",
            "KING RARCHPom MARTHARUBET:\n",
            "Ner swome brack-sine wich coal gamone bothe baight no\n",
            "smay peeat he tirds of way\n",
            "Your fatheled.\n",
            "\n",
            "DUKE OF MARCgAPUMA:\n",
            "There suble, shalt us is it gut a raight-veew, his my she knos Mo moth of thou broth-sennist to bunde\n",
            "Lour him my with thrus thars see,\n",
            "I, it feas was would do boid: I regay\n",
            "I ehath my and spost, If all he wi let man bust flieven: you yoursemear? Ritharm not with hthe is be tell sourt,\n",
            "this a thusbmarking bonief-son, gestrat,\n",
            "\n",
            "MARCIO:HARD I:\n",
            "I have I have a we shat\n",
            "Thy should of ropal'dinll my wrose\n",
            "Go.\n",
            "\n",
            "KING ON EWBRA\n",
            "\n",
            "IVMAM:\n",
            "-Jirse Yy the upprare; My trumphiced! him where I be eare;\n",
            "I'll as his fould the have gresaid merbs,\n",
            "You wixen my cittinfus that the you sping eith thee, that poochter's. Myspeove for bith they,'e I what more a well: thre\n",
            "train loss 1.8073, val loss 1.9534\n",
            "Epoch 6 generated text:\n",
            "\n",
            "Gauthere, both allir he turmen\n",
            "ortanus one thy gone to owns day,\n",
            "Mudhenced rabauth honour glanv-Whosly, courst facest, alved;\n",
            "To y four shaltst faclace? Rysance nibuse,\n",
            "At tidgues have king, a brines. Pond,\n",
            "\n",
            "Hend manracAte feales tell. Whebp, of secuch is not self.\n",
            "\n",
            "CROMER ED\n",
            "TRUMNT:\n",
            "\n",
            "Rike well thy beavy no seir'd, my contd werest honour to me fromace,--\n",
            "onle a Markes me; but a If invools,\n",
            "And do beef To the comen, and the plaspey my armphices! Tagle the bidy pury, Sizous\n",
            "To live to our hotors:\n",
            "And thy boid's that hen by broth biding,\n",
            "Indsure voldon'st I exttonly vitues.\n",
            "\n",
            "QUEELFOLIZ:\n",
            "With would, it? Jus, the puddesh, now flind\n",
            "than may noblem\n",
            "Or flown-bligh mee and raquet hoccane:\n",
            "That's brike that\n",
            "Your bathining? Row' thenuse ond, That you.\n",
            "\n",
            "\n",
            "CORICELILLAND:\n",
            "Kin glant,\n",
            "Rew But wars, such pone in you his lond.\n",
            "\n",
            "PARIOLANUS:\n",
            "Ayort, Is you care upon you mady.\n",
            "\n",
            "JUCOMNIUS:\n",
            "Ind a faway Sasumanty thhan be on,\n",
            "Side.\n",
            "Neo she fatconts I pave thoughnisan,\n",
            "Will a well slay spedce, such I have his w\n",
            "train loss 1.7867, val loss 1.9283\n",
            "Epoch 7 generated text:\n",
            "\n",
            "A cogaurt mede, one he detumn, our thou I,\n",
            "You aren here, howly proothina'd's Corman:\n",
            "Nor me which one ar hument hears no me,\n",
            "That fir you diss thrid in his afalf.\n",
            "\n",
            "ANTELO:\n",
            "Aleit must lone, gyal not in it'er pering af shall you has last her friend els:\n",
            "A beate no\n",
            "Uut, yet would if thir when word whil o's heav' out i' every by uncleius from live you.\n",
            "My falom, burce, where pleace, and thereibute alt and to ent, wherefore teach.\n",
            "\n",
            "AUTUS: Am uarned alay.\n",
            "\n",
            "CLord:\n",
            "What all Richap, speak: thou hath.\n",
            "\n",
            "Richalino, cannote ParvaSt on put.\n",
            "\n",
            "Prevest, facont:\n",
            "Clashing it an try levent\n",
            "With for werseas anate, prienceation,\n",
            "Then scate, who see you shall, what be sawain, beal;\n",
            "Peast it not no soult's a ginetly\n",
            "But a fead in cry sprotheain\n",
            "Bour't how abon. Wen net herself in of a ever\n",
            "And als Leget of our fain?\n",
            "Ohan's deather masted to so tell\n",
            "Than cone save nevery.\n",
            "\n",
            "Be, Lhead ase luse to have the is. you, sent uo engers,\n",
            "My shall you with the sard Reficeo:\n",
            "I his froke ar unter you, it thour naid urothe\n",
            "train loss 1.7568, val loss 1.9043\n",
            "Epoch 8 generated text:\n",
            "\n",
            "For hofficio, has give the aay of I man sick, as my Bad coman:\n",
            "Who, we must if to my damp, I'll delow rematter, Yord frome,\n",
            "Soo, now he many bure could the brow?\n",
            "Bur'de shous,\n",
            "'Thaw I'll indom han come again'st\n",
            "Wild her resontly eld sent that a how I will,\n",
            "Prace\n",
            "Be must be and the the preason, what's bold I knot.\n",
            "Thu be endy alted but tay world, Ricome,\n",
            "Be you, unljed do chiess heip mide cutage and the have bold be sable, 'able;\n",
            "You go ald thou was, what I nate me by the'e the bed one can to hany:\n",
            "I him mand\n",
            "thou Say hand lose unquermant, back,\n",
            "Se make mire hand you faresces cheganss.\n",
            "\n",
            "PULIO:\n",
            "O Ank o' egit end\n",
            "By so path, as be marcitis, cange, and be prepecy\n",
            "As SIelo you altien at beg frowe,\n",
            "I'l han the come with him? broth then?\n",
            "But not caire patientl, for found, to seep you nurt:\n",
            "I have 't, the's are if and I how party;\n",
            "Camlering and you, but stor.-\n",
            "Second with wild hou, from be chald.\n",
            "Ut, it he do bloods,; in wilt what make how?\n",
            "I vant you 'ell I so hame now\n",
            "To their and so you hou\n",
            "train loss 1.7419, val loss 1.9060\n",
            "Epoch 9 generated text:\n",
            "\n",
            "KING OF LEWIS RICHARD II:\n",
            "What wither some more gendmerbhect,\n",
            "Where ocly lifess agelved; me a make cof a fament.\n",
            "Brace your more deven have behing\n",
            "Blheard, I let like goac of of quide he!\n",
            "No, hide parce my becke faresher-baggate it is the guick\n",
            "Ung a their of are falling insmurder?\n",
            "My the shall payes; looks I live hand reased,\n",
            "This disess of meas? you bly him,\n",
            "And haut our dereade to but night you,\n",
            "Dudne boscing wi cameed.\n",
            "For it hen id mirrage you? Dukancy, how a ithe full undersonce brighs.\n",
            "Is made in us my bothere, recomman:\n",
            "My doock well mour me: agat in thy sency that the\n",
            "kefore you!\n",
            "I love tame letter of heaved? Lout it't us once is some\n",
            "-vire sa; uffor him suffelo twith leens-makeck'd;\n",
            "But I see same of a despesderange?\n",
            "Shouldom proye: u tell bucks.\n",
            "\n",
            "CORIOLAND:\n",
            "Her, whill in throught's fasty word, I crestires:\n",
            "It me past to legard, nor best; my bettens.\n",
            "Look! it canill here\n",
            "If will contley your make of of merturing?\n",
            "\n",
            "THARDUM:\n",
            "Soill you; and best anim myself,\n",
            "Examse colturn's par\n",
            "train loss 1.7217, val loss 1.8993\n",
            "Epoch 10 generated text:\n",
            "\n",
            "I no madefied bering beFrow thome,\n",
            "You no, my indeed, so souble thus of are That sham of knooth:\n",
            "That not no't, four marousby and beggelo.\n",
            "\n",
            "the I hen laidy and instrail?\n",
            "Gives in this arice.\n",
            "Yeavel the hide thou take undermies!\n",
            "\n",
            "Nurse, Selver And uthour! extsenere;\n",
            "Or while do hand were we traid not, but withes; I all my knowled the father! a mate one of ashaltes, of my not me?\n",
            "Now othir man: I age foul well, shown.\n",
            "I save bed envil one all and good peop to out peapine.\n",
            "\n",
            "ISABELLLA:\n",
            "For there, arge him, from, spake I withous nof agguille'd should preace profe,\n",
            "And a blieved hand fourier namhy havege,\n",
            "Who ed-fearing again. And fackew you sluing to poge\n",
            "Now, thou sever my son than my breater. You, for my own I save thou a name?\n",
            "Whou, leads not our so, I.\n",
            "\n",
            "ARTIAN:\n",
            "And me us the kand,\n",
            "I hus no with\n",
            "I dranged times I mouth my raggont:\n",
            "As his you sat\n",
            "Our the beels sover who oher show this ging!\n",
            "\n",
            "JULIET:\n",
            "To had bet, you ar.\n",
            "\n",
            "DUKE OF AD:\n",
            "Is I love, save not theer greastion:\n",
            "I--lares fumber, all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not Randomize the order\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 500\n",
        "num_epoches = 10\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# make same_dataset\n",
        "same_dataset = []\n",
        "for iter in range(max_iters):\n",
        "  xb, yb = get_batch('train')\n",
        "  same_dataset.append((xb, yb))\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epoches):\n",
        "  for iter in range(max_iters):\n",
        "\n",
        "      # sample a batch of data\n",
        "      xb, yb= same_dataset[iter]\n",
        "\n",
        "      # evaluate the loss\n",
        "      logits, loss = model(xb, yb)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  # generate from the model\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  losses = estimate_loss()\n",
        "  print(f\"train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  print(f\"Epoch {epoch + 1} generated text:\")\n",
        "  print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "Zi5AU8B2KZVt",
        "outputId": "e35e3fff-00b9-4e1d-c52c-7a37e8b015f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "train loss 2.2941, val loss 2.3043\n",
            "Epoch 1 generated text:\n",
            "\n",
            "CANCASDkRD:\n",
            "Ocow and O lay bermad tho bube toe.\n",
            "Sagrth my dalatanss: Vanthie ulqur, tot?\n",
            "F dilas ate arche my thanstar mzo mun\n",
            "Yowns, tof in he me mil nowlll, es iree-sengcin lat Hot drov the and\n",
            "Win nong ilerjes!\n",
            " lolind te litenser onchiry prup; aissth woy. whe nis nor\n",
            "To thig I whom\n",
            "I I the to kello Wind tour Ceiibke the mof srive cenghieng thoo to er; thu the blounrtht f son; iry tom:\n",
            "E wre inler ath, af Pre? grono my Hay sue!\n",
            "Ktied is:\n",
            "Sadsal the Ede sthidin cou asar tey Iry tof han thove yo no ton, bem gy.\n",
            "Yor I Ong my mesor ton hin.-\n",
            "\n",
            "KINTOUS:\n",
            "The mreand tho me lin sonn loug bod\n",
            "Ong much ffepy that thas lom my ias dethul ings.\n",
            "seaed Ped helllvet ar dinsst to os if mevet beie st ume histe feRil as ny, con longscern?\n",
            "Whown lou Ie neesoxl arer thitheak\n",
            "I agherchin cweal k sleve buml nom,\n",
            "Whos wllo nonch id, mche hI surd?\n",
            "Theid I thind no nond Ce? I mie thistou tiund tho nof the sut no iplyth on whand nong thers\n",
            "AndE Blowllke ton so han the ibed\n",
            "Bwat do ive wout ir foru; yorke;\n",
            "Gou \n",
            "train loss 2.1133, val loss 2.1334\n",
            "Epoch 2 generated text:\n",
            "\n",
            "Mon than bruose. fariveret Hank;\n",
            "Whuwh Rutch in the priveftand and\n",
            "Thoay hour worde be, mesevemines an te woreingin in sone to mut on word,\n",
            "Dodca not brane bencatoIn\n",
            "Be ta catint and as in to nableke in blaid at mur tre leing brwimng to shawe e would in to so thou for Howe\n",
            "Thamp lalll lop! tree of thee trjee' deer of thre hashe arling mand hin to his;\n",
            "The flot reand not thenve cour subser sefor pillbust.\n",
            "\n",
            "UCESMICETIOH:\n",
            "I Buto Butay him nonu to may meerte you thellge,\n",
            "And yie ou chat rand lart e\n",
            "Thin me booke Hewut blether, sare as;\n",
            "Rove me ofkave to feeres ontre\n",
            "At Ousheresed wor ristill\n",
            "EN ELONc dring? I KereLer knathe hipprivin.\n",
            "\n",
            "Where mave sher ar ther as I I kit aund courmphir,\n",
            "And as o'd gunt theat hawat lhern Welly\n",
            "My wiflll gire ma that cor courch tato not, lichoul?\n",
            "And court Of you ben' for Bus onot,\n",
            "Wer wis houlioner like he pusint\n",
            "I and to thee woullk: not fannur woury prour him\n",
            "y an noe trownather Vand of an, Jurthe panding\n",
            "Whis onst whourfuth thee suere one heherid\n",
            "Io \n",
            "Maru\n",
            "train loss 1.9905, val loss 2.0562\n",
            "Epoch 3 generated text:\n",
            "\n",
            "Be evere beliven, his lome ay.\n",
            "\n",
            "LARGHANUCE:\n",
            "She I mongull folsst you eaut barwhtere\n",
            "What recthur done is or kie worst Ondlay, foor I weire?\n",
            "The for bsore of\n",
            "Good witis it on urut nethere but tome ptword.\n",
            "\n",
            "LOUCY:\n",
            "Wherewilll me thou nepee, nou more here\n",
            "Podsuld saded, hould manunt whould soake\n",
            "To Enemadiney me whill non mant, upour hilds be.\n",
            "\n",
            "Cliven Leve, honst ar my prey, grand. I with offor on pared,\n",
            "Sichare and my the he lince ant this woord\n",
            "Iit fachy: trow he knoll knot ou sin will for his for fine.\n",
            "'Tinle, make:\n",
            "As to and thou would taight but good Ay tarp haver spe\n",
            "And man be ane wind set! hall'd the shat the thou reat\n",
            "Cas be or more thee his of of sleplouds one padty me worders.\n",
            "\n",
            "GRIONGBA:\n",
            "And of them it in would, and you rinerker\n",
            "Akence ar if me thou bithit en saugh,\n",
            "So byood to rown, and wond no clegeaver to kno!\n",
            "King have myine wing ing not reap;\n",
            "I ben cayner. Sony corthourd hust, of that her to brut as gesper an sen?\n",
            "To kthour I you my yhand heir deable'd,\n",
            "Is I would worl is t\n",
            "train loss 1.9162, val loss 2.0096\n",
            "Epoch 4 generated text:\n",
            "\n",
            "Shan conser?\n",
            "\n",
            "Which soft the fim Hostset, whereinj I her of insmy beest. I\n",
            "That in sore:\n",
            "Lefor:\n",
            "Oe:\n",
            "Seare will four age storn, thou knond God do the him rive!ion'd ahat hoursperes,\n",
            "Soman ormall, thes is and fail thle be you wild kin you his prowe his toe,\n",
            "hand hus ance ood man,sed ward the wordain it her bestrender home\n",
            "Aver it boie badone you father's\n",
            "Irehter Envorward I peong or han tarcusoch?\n",
            "\n",
            "KILIOTIO: Buood many exen to jur unathou apjong od the carce at xpoulf in at I'llow-buselid!\n",
            "\n",
            "HAMBENTISA:\n",
            "\n",
            "Do To alviounted, herest younhtarer.\n",
            "Igaid tome sither,\n",
            "The kiven her I sar That I stood mante ware\n",
            "On lay be teeve wind worlt it bentluared,\n",
            "And mine\n",
            "You hiH Whusped ware resarehtsub'd sonown: hads; ye\n",
            "Murie thou Anday seery full, KI lasse,\n",
            "I love ear and fainderter tentseren, bind recon.\n",
            "Buse, thiqust in him aftergunce!\n",
            "Hers himen to surtery.\n",
            "\n",
            "KISTAR$\n",
            "TILAM:\n",
            "Phor GigUmen! to I would not beded?\n",
            "I would bothes, anEf and Onburousie.\n",
            "\n",
            "ComTOUTEd Lake wroven, quon Sonmmour But my not?\n",
            "He my w\n",
            "train loss 1.8644, val loss 1.9750\n",
            "Epoch 5 generated text:\n",
            "\n",
            "GLORY Som:\n",
            "Your will prove man one, do bese and my in tell tiRe\n",
            "To crevist'er senvery petlie; soes, no Rome in the And blood!\n",
            "But I son centiore me comne lord in be the monglary's:\n",
            "To the Betterect love?\n",
            "\n",
            "KING RARCICUTIO:\n",
            "Tileminal orr sworn:\n",
            "Think in ourch coan gamen's,\n",
            "Preave in andushious not he to dainent\n",
            "To offul, will of, as is life gest.\n",
            "I have thy lebAst.\n",
            "Wither it en tor, I will sen, be oonesher you had; I\n",
            "Bot now,\n",
            "Fronsiter'd tit hat antasuong aug.\n",
            "\n",
            "PETULOUCENCASEy:\n",
            "To muse! hands: and to chean is rathrogan\n",
            "Arehter my ands foid, If Iff DIgin it you kbust;\n",
            "So of binke itely meargainte's to to castetts is bebesed\n",
            "To peart in a thusbetherery beien-son, gestrave\n",
            "the roo Histrrander agens I'\n",
            "Take is him an arest to mader, diell\n",
            "Thou prepurse his like.ount\n",
            "'nor Mat--\n",
            "way, you wauch arevery, romphione!\n",
            "Sone for old man you ruchall his for to manother;\n",
            "That his begeanest before cittitel.\n",
            "And hap a sonesing eithought, to her achis ti.\n",
            "Musheore, the it bushan'd I wall mooursinet,\n",
            "And n\n",
            "train loss 1.8369, val loss 1.9740\n",
            "Epoch 6 generated text:\n",
            "\n",
            "Gauthere hands allorkent?-\n",
            "To love and othior age; one of sheak,\n",
            "And you buranie; but not go ove you thy plistifuel'd calved;\n",
            "To you wourn must farwartor: saje? be thander of give is one to so be mee.\n",
            "Ponder it andeer'st long and teeming\n",
            "There from the lothere, it for foult indeace\n",
            "for more for the beaves? Is it an man, comment'st worth at faccort.\n",
            "\n",
            "GLOUSTEROHESS:\n",
            "Fext ofor Earward fore, mosteruy.\n",
            "I-f Torther to no Rome, it the by mentled.\n",
            "To encage.\n",
            "DUe, it lingstent: onbor one proacy,\n",
            "In of whieh tongbough-onorewer: by arwas borrove cousurelve,\n",
            "Have seee mangles bindsme us not be in most cannost pued, hine bed to be Pingsted\n",
            "Os wratoon begmans foolat luashme oanter?\n",
            "Heards, and the man? I'llot ve\n",
            "You mantliaste?\n",
            "Which mone you warTh it andrust begcher elt wath I'ext,\n",
            "Rome's lords, and it thou wool$\n",
            "Andles Gennery my howmecore, I legeach owary of thy his holin's\n",
            "Withen an for I Sasue?\n",
            "\n",
            "POLINGIO:\n",
            "Four that.\n",
            "Ner'EPst, aconts I pave but annot word?\n",
            "Than:\n",
            "Kine the pople, save I hourthing \n",
            "train loss 1.8133, val loss 1.9553\n",
            "Epoch 7 generated text:\n",
            "\n",
            "A ange race deatoon he detumner and one I,\n",
            "You aren he'er hould; roong take's to mest some with my lave:\n",
            "Liet as manta do me,--\n",
            "Castirry was the eard in his asalf.\n",
            "Hatood sown it but connobless, not in infor preend.\n",
            "\n",
            "SLay:\n",
            "Eive this astiled fou be els: buick and four,\n",
            "ISABELLA:\n",
            "Till me broke all while; so be reave ingeven, buturn.\n",
            "I old mine they here happomen man,\n",
            "Which you saud, git a but the chanderel\n",
            "To the engure: it a wellouss!\n",
            "Wherear wonto pulace; foll mane thou hapore man:\n",
            "Thonou earsholy alie. Whank whola vantley put onarewell, gand friely him.\n",
            "Now', srowle to frieve that I was anatoon of Yorant, andearts\n",
            "bed, wrontle you shalf forom be sawear, beal; and so?\n",
            "Horven foolet'st but below and are\n",
            "heir crupst my baint of 'tearwings how,\n",
            "Lort herself indom an you\n",
            "His bose To see, phomegh; idear's do worn mested\n",
            "Bust as maine his no dabvang own.\n",
            "\n",
            "Peiver Mown.\n",
            "Is secrove:\n",
            "But swool's knes buyut enger and the be your.\n",
            "All me sore Refives:\n",
            "I one arnore whose all and juidger?\n",
            "\n",
            "POLIXEYA:\n",
            "train loss 1.8047, val loss 1.9458\n",
            "Epoch 8 generated text:\n",
            "\n",
            "For horrier then Anfule. No an of I mands,\n",
            "Ther jurbedion; and the how mant if onempolaspe.\n",
            "Good! What Tome well come; put is hen there ares buience matuse or you shoue,\n",
            "For will he king ionforaluaced in\n",
            "the mesteding hir soltive his and that a holther on,\n",
            "Praces enecounted henour and the I lived to of maker.\n",
            "I would by endanabous bothy:\n",
            "Foor watcome in preventauch, my have ency.\n",
            "Gove digendage and him have wold.\n",
            "\n",
            "PERDADIO:\n",
            "But woot of ale\n",
            "thou wast it miren?\n",
            "I see not the que, en baider His hand'st,\n",
            "Wi mance hove and mentlest uncupore:\n",
            "If and to jeather in he forgrefores then bratherra by pring'd wo feediness\n",
            "Buarshalth, as becond, be wren than be Enarrece:\n",
            "A counteor! Italand natwarger;\n",
            "Oe pardiner to did of the erest;\n",
            "Now then all revery; word, in no go the encelyous\n",
            "And no no thee thy is ontriabent:\n",
            "Heray as honourabbed and puirest\n",
            "There our should soulf it hourder, but for mine good out,\n",
            "And for Peastesh; in wA-mown? have you will the made bed; and your now\n",
            "You; life it so this on\n",
            "train loss 1.7918, val loss 1.9520\n",
            "Epoch 9 generated text:\n",
            "\n",
            "KING OF LEWIS raw smill how weedits in to-prower\n",
            "To me beecondere woocu slet more unwod;\n",
            "And we mantethan angraids home fired,\n",
            "And ven have behines ladyiges I knal,\n",
            "Let an I'entrely: if eStand to Mpary in the buife\n",
            "You well, as  it I hing a lovear'd.\n",
            "There offarecowe is wifed their is to u have proun;\n",
            "I would I love mine; and but him; exer,\n",
            "To a Castencly it on which as offulf and to be\n",
            "Think'd my ercentible one to chiled.\n",
            "For it hen idguine, wife whelvator: how and?\n",
            "Sef the will in therefore-I?\n",
            "\n",
            "ISFORCEO:\n",
            "Goin all endremigh, whose somen Inest!\n",
            "\n",
            "BALLO:Onto, I by sovewer, thou king as young withincem?\n",
            "Is as Woary offe? Loud, blide? I woulds, bre--\n",
            "I hon; wifld him sufforowerity letch-parder'd; in buy\n",
            "Persing other no.\n",
            "Sorong Somace:\n",
            "Some 'tis is all kind's in, I Love appering. Allive, I\n",
            "whith he, sont my blacke: foer you know thou sing brack rooon.\n",
            "How I'ltans owningsitence.\n",
            "Alo, I'ear me loody, you say thy dather to knible\n",
            "Tentle teneritly one, amoves take I limpies boode cole.\n",
            "Too pur\n",
            "train loss 1.7814, val loss 1.9529\n",
            "Epoch 10 generated text:\n",
            "\n",
            "I now dine have renot, For, thou whose which first art were be stall of which buse and\n",
            "To doot and he woven asweord, manour\n",
            "And I befth, fartherives, laitt and in the\n",
            "berchive in the see.\n",
            "Is as before love in thrian! I Ricke in hopate,\n",
            "Solt may knuth forpeous care; arry how did he Rompett his idearn, but wood\n",
            "To bid seen in old not the gile a maturn.\n",
            "Proves, before me, heriant? I' keep reakn:\n",
            "I againtain, Welcondently\n",
            "In abb'd enessage, and and good pear oor on platine.\n",
            "In It would a Jighir, fargether,\n",
            "Truand a letepwick me no down in fauggest,\n",
            "Beadining with injuage in the be? would O' a his.\n",
            "\n",
            "IgRIO:\n",
            "I, Iffersire right; thou fant piders:\n",
            "For to inglaided thou we'lt it bes in my lay,\n",
            "I know you have troyved saughts\n",
            "But not to louds I know borth, I cuse him, to be are upon hastrour why redsen line puchad\n",
            "And Lord onour for grown:\n",
            "And is you a to child, be that we kinlowh the'' weal holave\n",
            "Tohts a thou arebit, one to live,\n",
            "Tearn no when to drafter kintUler of melieve,\n",
            "By ands our him But\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Default\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 500\n",
        "num_epoches = 10\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epoches):\n",
        "  # Randomize the order\n",
        "  random.shuffle(same_dataset)\n",
        "  for iter in range(max_iters):\n",
        "\n",
        "      # sample a batch of data\n",
        "      xb, yb = get_batch('train')\n",
        "\n",
        "      # evaluate the loss\n",
        "      logits, loss = model(xb, yb)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  # generate from the model\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  losses = estimate_loss()\n",
        "  print(f\"train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  print(f\"Epoch {epoch + 1} generated text:\")\n",
        "  print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "I3oi0ml3K9TT",
        "outputId": "1a3289cf-a0b6-422b-d0d7-615c84859555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "train loss 2.2941, val loss 2.3043\n",
            "Epoch 1 generated text:\n",
            "\n",
            "CANCASDkRD:\n",
            "Ocow and O lay bermad tho bube toe.\n",
            "Sagrth my dalatanss: Vanthie ulqur, tot?\n",
            "F dilas ate arche my thanstar mzo mun\n",
            "Yowns, tof in he me mil nowlll, es iree-sengcin lat Hot drov the and\n",
            "Win nong ilerjes!\n",
            " lolind te litenser onchiry prup; aissth woy. whe nis nor\n",
            "To thig I whom\n",
            "I I the to kello Wind tour Ceiibke the mof srive cenghieng thoo to er; thu the blounrtht f son; iry tom:\n",
            "E wre inler ath, af Pre? grono my Hay sue!\n",
            "Ktied is:\n",
            "Sadsal the Ede sthidin cou asar tey Iry tof han thove yo no ton, bem gy.\n",
            "Yor I Ong my mesor ton hin.-\n",
            "\n",
            "KINTOUS:\n",
            "The mreand tho me lin sonn loug bod\n",
            "Ong much ffepy that thas lom my ias dethul ings.\n",
            "seaed Ped helllvet ar dinsst to os if mevet beie st ume histe feRil as ny, con longscern?\n",
            "Whown lou Ie neesoxl arer thitheak\n",
            "I agherchin cweal k sleve buml nom,\n",
            "Whos wllo nonch id, mche hI surd?\n",
            "Theid I thind no nond Ce? I mie thistou tiund tho nof the sut no iplyth on whand nong thers\n",
            "AndE Blowllke ton so han the ibed\n",
            "Bwat do ive wout ir foru; yorke;\n",
            "Gou \n",
            "train loss 2.0994, val loss 2.1313\n",
            "Epoch 2 generated text:\n",
            "\n",
            "Menron, were for, lod I Qull no;\n",
            "And den the cre, heer this to nonge bayte or wagadsen, mese things sevee wortingin in-s beaight:\n",
            "Hon wow nrechcald bre good hichenIn am-sa thein cat may, the an be ketim blaid attell tre leirg brem nhis wall see wo I beavt;\n",
            "By thou for Heaing taserel brasp!\n",
            "And cold crest, beake fran, throu therfur;\n",
            "As mastin nir il cay me. lotir Wend hlad speay murcs bend sey.\n",
            "\n",
            "Nullb, the to gake the bae swen henaure tan nurt om The free your.\n",
            "\n",
            "LUENIUSTA:\n",
            "Mey, And Rarne ulets enerens; sin jey.\n",
            "\n",
            "Mught, mer, sages so.\n",
            "\n",
            "The thouk vis scace:\n",
            "On! preaete in un seflle, ris is\n",
            "My en wacld beir sham-dLery.\n",
            "\n",
            "\n",
            "NGet prrivin.\n",
            "\n",
            "WHARIUST:\n",
            "Huld. I whill as I will in he courmph,\n",
            "I by as thourgh's these haw welfernd ent,\n",
            "My wifll this fat arch:\n",
            "Hrace vof tito thou lichoul?\n",
            "\n",
            "QUENESARTOR:\n",
            "Ge the ase all cooordt.'T: I litherower lik;\n",
            "Te punict gungou, and ward; wat nat fanns yere yeng my,\n",
            "They and,sent, Juarbatusence stan,\n",
            "Jurth woat! yer briks shoun'ed.\n",
            "\n",
            "MARKENIO:\n",
            "I I he hehen be a antir\n",
            "train loss 1.9754, val loss 2.0340\n",
            "Epoch 3 generated text:\n",
            "\n",
            "BOLetlant, liven, his lomen your's\n",
            "stinles of herem high me asst youre,\n",
            "And the enecrdinir countlound is tooke of lott to couckem ponest,\n",
            "The mand elsore of hall wimis it of de\n",
            "Thou he no fad you kthand is cour be mance recke that nep.\n",
            "He tham youse encorsuly:sed! Menctly manunes go yourease\n",
            "spirn, ad hey me whils rone apporuce.\n",
            "\n",
            "his sobe by, dise me, knot shall my sir,\n",
            "the sea he dous be ress pared this,\n",
            "Wellow mente markly cinnet for aws:\n",
            "If it fachy: groht wekesles for oure\n",
            "so to-qut cemane be hack, lindw,.\n",
            "You kist fore sit lew beigh hick bring\n",
            "say tanke you blarcess? D, cannomee wind set!\n",
            "\n",
            "DUC'E E VI Latort oond were you ronger as mackee yount andss the for bee padty me we weirge, icloing mancuose thance in we\n",
            "hind me your ben kires with ar is me this bickit shind, her of\n",
            "And and rown taghis and Buchegen\n",
            "Cim a knonly aphours, tixh gun in Done\n",
            "the prome Good you. Yet you ham-ded hust is be Cere;\n",
            "Pencorty as grup thrunks you'e the we by to Rikh, that jrace,\n",
            "his stof his are by his t\n",
            "train loss 1.8998, val loss 1.9877\n",
            "Epoch 4 generated text:\n",
            "\n",
            "Shall the beem hisfold, it welim the set,\n",
            "I kquink where of incluin thin are thou whose:\n",
            "Letsery and ware willaight age sto ort?--\n",
            "\n",
            "RUCHy God drights, it you infull hame o'es\n",
            "\n",
            "Fere ham I wram not bestill happs\n",
            "bold greckem, whep who your baopeds where heaves your rapce to mesarss, ware the with tell not; bust like cany mever it bries.\n",
            "\n",
            "Dothy:\n",
            "What, ampetron extle dowa whelp I meere with a Pagchy in owell wild,\n",
            "Un platesen that fillat o' apjon wouch a carcest mxplits in all we swould lid?\n",
            "\n",
            "HAMBENIO:\n",
            "beholy fraloiesn, foe, erear, bence uppatged a devess buckslages, and the! say malm mastle. I eyboramed, with bret evell no whalt is benglur,\n",
            "The crielvats a malandurp; baterm hoary.\n",
            "\n",
            "Prikes. I he hady not\n",
            "Muriest, of maay send suught, colasst, soy impele chervartuert muted serengs, Where; jeo now, thiqust moirs and!\n",
            "Agunce! Hers him day woulded or you cont;\n",
            "Yull dive cigselamet but would his eded?\n",
            "I whelp botch woulEf ares thuse\n",
            "Willie bencaspair me wrack; equen Sommen had\n",
            "with; sue calmy w\n",
            "train loss 1.8227, val loss 1.9445\n",
            "Epoch 5 generated text:\n",
            "\n",
            "GLORYCUTY:\n",
            "Yet full be fry my vorian's buse and: tlier, I leke Eng zineds\n",
            "'Tins\n",
            "Him, but aidy soes so\n",
            "He brain the starkly scref\n",
            "This sound is Agare fent lord it be the morm'ory\n",
            "To puch exctuter\n",
            "That him bust of my soulk, sil; I an her\n",
            "Of thee live in ourch claw wamen' boche baiid and\n",
            "Ummon very helpings on, and far' man'ord.\n",
            "\n",
            "HARD IIUST:\n",
            "griven, there surle, and with is it gut a,\n",
            "And all the his dersher no morrie I wereld,\n",
            "Fronsiter is it hat ant. Auchicul.\n",
            "\n",
            "GEN VORCENCES:\n",
            "\n",
            "You, awa! hands: and turch I his rathroga:\n",
            "A now sry well, for to foul the wo let my rencerfl to heir\n",
            "Meicels mear? PBut is but with at soil's beelov-not,\n",
            "This\n",
            "Is thoubmanion you? en--\n",
            "The horrie,\n",
            "\n",
            "MARGTHOH:\n",
            "Now:\n",
            "Wert thy shall wo dorry\n",
            "Thy are futhim our cainlly your she, suil ady a bound\n",
            "And lant--\n",
            "why fy the uclanest Mard upon offilliness brottione your 'tus hand\n",
            "dot; to He hard; whithy berpole\n",
            "And no for thithine suchall fright you, beine tide, thou poochts tire as cornfurly itizes to'e\n",
            "Thece buch I diness thre\n",
            "train loss 1.7892, val loss 1.9063\n",
            "Epoch 6 generated text:\n",
            "\n",
            "In they jodnot awerr he turn mingorn.\n",
            "\n",
            "LOUCE VINTIO:\n",
            "Nor me dare thine long; take but thy graiv-fathl the kisting,\n",
            "That father to hath her most farlay of yiard?\n",
            "\n",
            "These I the dauge have kit: so bried he sid\n",
            "Whith much her long and temmink\n",
            "Them of think him lese, she fir haved\n",
            "Fanur the rame we lands-beapy no seir and to, coth graight, thou then fromake,--most your heart dant lard fair onf mostled life here be for no about do mady\n",
            "That I had in of agling me! duling ment:\n",
            "Coborienuin soul, to of this farthbout: on arwer: by art soblers.\n",
            "\n",
            "QUEEN VINCENTIO:\n",
            "Well:\n",
            "To losdifed me, stands: there Le cans,\n",
            "And Jul, there fatth, now flifert wher?\n",
            "\n",
            "TOLfOUS:\n",
            "If oul-Nor shall and raingt good weath,\n",
            "Thouke bert very hamb think ying love one.\n",
            "\n",
            "Fill That yours, the gore felty and glant,\n",
            "There all dises down the bute? I kely nothnur,\n",
            "Istogim corn, fore lack owalt of thy gives him'd\n",
            "With night ower Sasuling dayht, but not is feat\n",
            "'Es fortconts I pave thoughn saw, midilar: at flaw trefort speek ling think \n",
            "train loss 1.7630, val loss 1.9016\n",
            "Epoch 7 generated text:\n",
            "\n",
            "A ange race death, the doth ner amstorne;\n",
            "And, agent goves him.\n",
            "Brood is of sive me's so, seend my lay and ut high lord him them suffrinds; all thrigh of is as:\n",
            "Which owe stelfice\n",
            "Wo clonobless, and long for for not firsan.\n",
            "What be last If herce:\n",
            "Tell:\n",
            "A be to no\n",
            "Uut, such of a\n",
            "Righam your know-make oogs he, 'mout ingeveny by uncled old mist the sheek high'm.\n",
            "\n",
            "DUCIIETELO:\n",
            "Duke us, gives buch Re's that ever him of a minclous!\n",
            "No:\n",
            "bisty APPusine, alay.\n",
            "\n",
            "CLARIARE:\n",
            "For that hapore make thou? geard me have himsend he Parval him putch\n",
            "ard Lord! a me! in my line heave roale to friend hir me.\n",
            "Well ady, pricharant shall fescal,\n",
            "Inded love, I him go hom be saway:\n",
            "Ayell; and so? no, go lights'd a givetly\n",
            "But our louk\n",
            "And prows not of of 't him exce. Ednongt herself. Till a cost\n",
            "And blood get of hold,ghand hath, doth:\n",
            "Comes time should agains concly thangens,\n",
            "Where hows dose ruse go have bes is ince,\n",
            "This us every acconster-now it our me;\n",
            "Shame fichom fiolt fonk, a hows?\n",
            "And I it thour name knothe\n",
            "train loss 1.7277, val loss 1.8691\n",
            "Epoch 8 generated text:\n",
            "\n",
            "For hofficio, in Agiund. That to plumplesity, as mouth mane!\n",
            "\n",
            "COMPOLIZAPUF:\n",
            "That we saway:\n",
            "I to self kiffe, were come; put is hing;\n",
            "And many burpect matters richafs oue,\n",
            "And wither, Whichious thrumble sage fortunn, shall,\n",
            "Wouth telqust too, heaver the not,\n",
            "Praces to coulf, and soqualt the wort:\n",
            "At 'Fors out knot.\n",
            "ThusIf exdeade to bethy:\n",
            "Foow way, you sopechess.\n",
            "You make come, yetiman dows tage and shall confold.\n",
            "\n",
            "PORDADIH:\n",
            "What vition:\n",
            "I know\n",
            "SwagET:\n",
            "His fnorsce, by the'e the compond sent\n",
            "To reavy's cise to\n",
            "Anongety heat lose the pres:\n",
            "If am no time to is: he's\n",
            "Agref lesces chegat sort the might king eyou and\n",
            "By sonper, I'll kiffecitions anged.\n",
            "\n",
            "KING RICHARD III:\n",
            "But not endid not beg frome, and hearty did mave sher?\n",
            "\n",
            "YORULIET:\n",
            "And ir you dister, if this the.\n",
            "\n",
            "AMILLA:\n",
            "It that nult, out by sto.\n",
            "\n",
            "LAUCESTINCES:\n",
            "Ay, thou doweed my pupomes\n",
            "The prous stoo.-Now thy wive with you, for like have out,\n",
            "AnId ducke stoer; in with with moad,\n",
            "And I vones father, I'll hass now\n",
            "You; like your fiel hon\n",
            "train loss 1.6907, val loss 1.8486\n",
            "Epoch 9 generated text:\n",
            "\n",
            "This will hars than that, dewell to his to mother,\n",
            "Have meather I woock my less agey: do's\n",
            "Tame many that countios\n",
            "to their honoury my have biddery:\n",
            "And do I let limandom so:\n",
            "It love, holy fher dear plower bechiofal him\n",
            "Obreway till us neet look, a guingivant's less; if withdure of\n",
            "My to w hals prouchil our flurn?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Trutes foo, alo?\n",
            "\n",
            "LEO:\n",
            "In tell ware us;\n",
            "No do you now in an o'e thy shoundiblechion we hooed.\n",
            "For it hentrealious! But wheread:\n",
            "Why wit ishe full uppose. Who;\n",
            "Henself? mad: if that but thee dratest,\n",
            "And your here the grive: abut in thy should hathts you hate on!\n",
            "Tway the marest Ban are of manious\n",
            "I like an would so quee\n",
            "I have unclalming down-pitiful lies.-\n",
            "Thecumand Besul\n",
            "\n",
            "This light hanger brows thou to good 'man:\n",
            "Fhalt, buch offricenous\n",
            "Which the silin. Now\n",
            "Fith her son try, I cresafue: who me a shot:\n",
            "In all my roots, now and mesom--too it canickle, has wither's king,\n",
            "Upon thy dit fiffuls in all petityens,\n",
            "You pard amoves take these in as my follow'd with \n",
            "train loss 1.6842, val loss 1.8236\n",
            "Epoch 10 generated text:\n",
            "\n",
            "A no madne have renow, From them 'twoe; 'Hidy in as busI souble the housauge,--QUy seectled of anone.\n",
            "\n",
            "MEGRUTER:\n",
            "The marour younjed forth.\n",
            "\n",
            "theUINTUS:\n",
            "HiRD Indeeks.\n",
            "\n",
            "DOGSOHESS:\n",
            "Yet she! in they before loved\n",
            "And reworn of Rijen! Copare enelemman:\n",
            "Gut, for exercoure; prusing\n",
            "his sheple metwert, From apple: we instemings me in old not.\n",
            "And.\n",
            "\n",
            "VORSTENR:\n",
            "And prawe, and to theire and him kither shappy againg:\n",
            "Meenercanoun thy and bed ene-says,\n",
            "All and good peop Lord, 'Timple a desen weing a Jigherpherd,\n",
            "In with a Marpare spaice.\n",
            "\n",
            "FRIAR LADY:\n",
            "U' to seems in him for which a blide Lexame four cOldeed. O VIgnoble and feeper\n",
            "Try, may the fanto yours: in-e'ting them,\n",
            "O done. 'Tis you, fistrom abse. The secate:\n",
            "I trows I saughts a breadath leads; and the Jure\n",
            "I cuse him them craude. I leady a cowl order---Heart\n",
            "Leaven insworn out neeAy to.\n",
            "\n",
            "GRUSTEREMIO:\n",
            "A to child, be that weak uponh the'st take give feed the theirs cbettly that lies,\n",
            "Gested with, loved.\n",
            "\n",
            "DUKE OFtUGHERVIO:\n",
            "His fat--O\n",
            "\n",
            "Sixerming that\n"
          ]
        }
      ]
    }
  ]
}