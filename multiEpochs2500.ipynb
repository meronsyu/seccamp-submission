{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meronsyu/seccamp-submission/blob/main/multiEpochs2500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "7f4654e0-b0d0-43b0-b890-2f793326b179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-18 06:32:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-18 06:32:51 (25.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomize the order\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 2500\n",
        "num_epoches = 10\n",
        "eval_iters = 200\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# make same_dataset\n",
        "same_dataset = []\n",
        "for iter in range(max_iters):\n",
        "  xb, yb = get_batch('train')\n",
        "  same_dataset.append((xb, yb))\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epoches):\n",
        "  # Randomize the order\n",
        "  random.shuffle(same_dataset)\n",
        "  for iter in range(max_iters):\n",
        "\n",
        "      # sample a batch of data\n",
        "      xb, yb= same_dataset[iter]\n",
        "\n",
        "      # evaluate the loss\n",
        "      logits, loss = model(xb, yb)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  # generate from the model\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  losses = estimate_loss()\n",
        "  print(f\"train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  print(f\"Epoch {epoch + 1} generated text:\")\n",
        "  print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "20450af4-5b3a-426a-ea8d-34c88007a8ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "train loss 1.8236, val loss 1.9471\n",
            "Epoch 1 generated text:\n",
            "\n",
            "\n",
            "ANGABERLA:\n",
            "Ochy and is by be madise my be donganthred my dagaid\n",
            "her barthien's hath budy to lascanes wick my thank's wize mus\n",
            "Young, toffigne into standing, and is ensen cintlatiHer,\n",
            "Whrute, and Will manightrans!\n",
            "All in you here she concery: the havest, why holven's not\n",
            "To this destomeny would thake on inso whruCerings with\n",
            "For His The his now pood of his butt kindn,\n",
            "Turt A son; wors! must whe know ontates, Prive my of trangiuse!\n",
            "A, you say ads beare Edwell hoin cour accanny ir-bust have for his no to skeem grives.\n",
            "\n",
            "WARGIO:\n",
            "Who, as eyour me? whath we men.\n",
            "Warwe; stare in with the condent of conf some shout might. And as we lapeings.\n",
            "What I know toove Lad?\n",
            "Casst to descoure'd trowe so not he be fear, as not comClongs\n",
            "Tunking to loudde neesoxtole.\n",
            "\n",
            "AUDIO:\n",
            "My badwer, but we mes slevy luel no's do so lack.\n",
            "Our id, more honsury?\n",
            "Their have brother, drue?\n",
            "Fy seven, to meius the should eysure\n",
            "That you suish hown hist coust this be bame of so hare apout.\n",
            "That I low Yought in for Gly thee boue\n",
            "train loss 1.6727, val loss 1.8232\n",
            "Epoch 2 generated text:\n",
            "\n",
            "Messuade it we seals, I hussing whe when the varther, not\n",
            "tanound withatter she\n",
            "galls be resentings so leads\n",
            "Singin inf to highved fallow now,\n",
            "And frienged brickedI haste at the callove, the any inkeep,\n",
            "To menatte intend eire breamn'd my have eath I dispord detto your Hadwas too lall;\n",
            "As there's life not jurked friend;\n",
            "Maught?\n",
            "\n",
            "Second Norford Casilic;\n",
            "The florty done hold my lick the begrad\n",
            "with volbidle I have plave. And I now;\n",
            "I the pernupty me wife teny my lelige,\n",
            "And yet of Rome one oldier,\n",
            "The smonvoling swould plock, say, as;\n",
            "Restrut of a-father!\n",
            "Coun! prove! O wherefore\n",
            "Of ristral bree wackers in on my Lary.\n",
            "\n",
            "Thy happring worrother:\n",
            "Wroud unagetiful shoulg, that been\n",
            "from his morastion;\n",
            "I'etises shall blay noteng hig wifl\n",
            "Of me patiful Edwrace voict, o'trung 'stolly talked\n",
            "And the wince afool gay of at burself? 'like i'light! Counscrown.\n",
            "\n",
            "DUTUS:\n",
            "I would the Lordens your yong, basting and seas,\n",
            "But hath and will on the Ripardon.\n",
            "\n",
            "ISALETELLYC:\n",
            "Why, and know me offeather, who hatis\n",
            "train loss 1.6119, val loss 1.7692\n",
            "Epoch 3 generated text:\n",
            "\n",
            "Be empanion, track: bitister croting in timen.\n",
            "Gry think me, so the dead must have\n",
            "And throcting do.\n",
            "Citizens a wremeth, love flower?\n",
            "Thou? best beens regody, you sons\n",
            "Approcue tone heave father ofthough yound\n",
            "The many holy fite tone there lamor, sufe\n",
            "Pray ince! Clown, lestnery's good astake\n",
            "To Enward's your while;\n",
            "So maputio our halesd'st your sligement cost a bretting, grace. Wellow\n",
            "I have stant home but enather joy my old\n",
            "city, sit I well, as desery: troush densled formorest oft--queen;\n",
            "see before help livives have to a swear?\n",
            "\n",
            "JAPhALTA:\n",
            "Saving how Atward' the crick\n",
            "And charrive having set! had 'd the shat thou prewer than rong eague.\n",
            "\n",
            "MENENENIUS:\n",
            "Seel thou showe the tongue? well.\n",
            "\n",
            "GREMIONA:\n",
            "And offecemen, with, his come, sir!\n",
            "O known with me is, therefulicious ansmuse,\n",
            "So by him 'trive to the LancBYork'd year a knowlinghmours,\n",
            "Till bun in Dongue ene of bence you.\n",
            "Ah, you havow with to jury Cerilling briment,\n",
            "I be crince you'll'd her by the grhadost preder,\n",
            "Before in infecendin is t\n",
            "train loss 1.5690, val loss 1.7350\n",
            "Epoch 4 generated text:\n",
            "\n",
            "Sherewn, we am hisford, it will\n",
            "And take, wherein a sheepicious to jest. But thou withat cause:\n",
            "alth in, I know with therefores?\n",
            "\n",
            "ROMEO:\n",
            "Ay, if your hater a tick'd ahalry!\n",
            "\n",
            "BRUTUS:\n",
            "Here's ragent, best Marcalister?\n",
            "Mercutymand, and not of Marge.\n",
            "\n",
            "KING RICHARD IIII: ancase!\n",
            "\n",
            "MARCATIUS:\n",
            "Nay, and if this head.\n",
            "At like many mever it: the callot all.\n",
            "\n",
            "COMINIUS:\n",
            "Harge with a whalt I meerch into much her\n",
            "a greates of him diagest king.\n",
            "\n",
            "LADan:\n",
            "'Taps, good the care gavex of fairl thy look-buse,\n",
            "Stay'd thyle newlies fralsiden: for not\n",
            "Are this: I wasged a devisitagenely, and of the read\n",
            "That I say thee. How means lay-breaken of not:\n",
            "Blooish-enge all to colouratek, Provadule;\n",
            "bandem his ahpoor'd she way affy not\n",
            "Mearer to beateds me, sound, King hearts your earth!\n",
            "\n",
            "Darererter:\n",
            "Hadst thee, God, I judgen\n",
            "A that strificion freel not!\n",
            "Here head, tows: tell me thy hath till drock,\n",
            "And I thou hast a grace. That you against be a will bodd: well I it be your cold.\n",
            "A busoves no done, and my grief.\n",
            "\n",
            "HASTINC\n",
            "train loss 1.5488, val loss 1.7431\n",
            "Epoch 5 generated text:\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "BUCKINGHAM:\n",
            "Him Ticklandom, sir; and like, know, countys\n",
            "His beins! wAy, peter's soonys had but.\n",
            "\n",
            "WARWARD:\n",
            "I poor the, sin thou sraight.\n",
            "But love it be the mother, his poishetity you cruption.\n",
            "\n",
            "WARRD:\n",
            "I know, sile that berrefreweer: why, or accack, and\n",
            "Englow your oid and summering bed,\n",
            "Carkingn at Yorks it 'take pursuised forge,\n",
            "And there you shall be use sitter to trute; and we his deare to be so woo\n",
            "Be relish, with winistice; welcome one\n",
            "Turre intorsral thereform, awair-fethings: turn you courts,\n",
            "For his hath my lords: it, I warrank it is thee beseaft to heir years\n",
            "shall gay that that was wet so sacreds\n",
            "Than dearth,\n",
            "Is thusbeth corsuin ensume, gentration to to vices and untormship our liserance, and friends of you'll my wrise\n",
            "Go.\n",
            "\n",
            "EDWARD:\n",
            "Now, o'er love--iwrongy the uclase to joy of him.\n",
            "\n",
            "KING RICHARD II:\n",
            "Now, curse hath doth tend, or degrysall beretween\n",
            "For every citying word: ha! That suit being thee, that poor this..\n",
            "Myshall fack to is shall gracious of Angelo,\n",
            "And n\n",
            "train loss 1.5191, val loss 1.7107\n",
            "Epoch 6 generated text:\n",
            "\n",
            "Was thy jodness werrected and in that know thy gold?\n",
            "\n",
            "GLOUCESTER:\n",
            "Will your lord; but not give me prith,--\n",
            "Death, there cheek. tell thou ast for molacion\n",
            "You are in sacret of mure is my first, bring of Endle feward,\n",
            "That wonded him his king.\n",
            "\n",
            "EDWARD:\n",
            "Bolity tell her is paisting unto oft-hate; and the guy nows:\n",
            "We will constang high on song feet.\n",
            "\n",
            "ANGELO:\n",
            "ondeed Marst madam, what fair only:\n",
            "And do. I am returns our oath the plasper that had in poing fair me;\n",
            "Eling my kinds of one than your to so\n",
            "were the otherforary\n",
            "And black and brother? what vokes 'hone:\n",
            "Antogly viour me, since betime\n",
            "Is confe, it house the peedes of Pitorth,\n",
            "As wrathon begman tesolary upon!\n",
            "\n",
            "HENRY BOHN OF GAUNT:\n",
            "Heary.\n",
            "\n",
            "ISABELLA:\n",
            "O hand this destal to more your will it.\n",
            "Retude begcret metus, noble thing.\n",
            "But warr, your content. Where gland.\n",
            "\n",
            "PARIS:\n",
            "Now my many fool lace own, in any my baster,\n",
            "Or murder a fower Savility darhangbroke.\n",
            "Sirrah, wisher nacons that, my voicent and mightar:\n",
            "King the people such a forgeting \n",
            "train loss 1.5115, val loss 1.6998\n",
            "Epoch 7 generated text:\n",
            "\n",
            "A cogmfrice deadly the deturn, or stoodem,\n",
            "And agane gove this tortoran and comforts as the city.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Was he with Capare yourself their courrse,\n",
            "Nays to more two it buckilning good the longed.\n",
            "Ourself affrance you this assues and furies the bretched\n",
            "Up plexsent to\n",
            "Riquarries and Jame Elions: I'll out i' the ye,\n",
            "Turly, I am mighting again headsman:\n",
            "Where lessely saudy, and if obeted,\n",
            "And feeminet, we asside teach.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Talk up me the tame? The miday, speak: thou been.\n",
            "\n",
            "ROMEO:\n",
            "I had not will valk-heaputed are Lordshall friends,\n",
            "The head or all to friend that the sastate\n",
            "Brodand and shame fulched, wroth I smiles this holy messenger beauthangs of noble well.\n",
            "\n",
            "First Servant:\n",
            "Winded this cry sprote and coursel take of then need,\n",
            "Sonced news and she who lies of down homeshiniorable death.\n",
            "More this should be prescenely tranging,\n",
            "Where chosed senseshire had a most findetes it upon thy accontessing are old me sart\n",
            "Repicion finged thee are woful, and justify 'God keeper\n",
            "train loss 1.5009, val loss 1.6865\n",
            "Epoch 8 generated text:\n",
            "\n",
            "So for your plensent to his and did man siste afterb'd my barial please honours.\n",
            "\n",
            "CORIOLANUS:\n",
            "Away; Lird, the rescide; side thou looked a courselve with thy richals our should\n",
            "'Shall bewith from from that my meann,\n",
            "The ironour'd throse thee heavy stood or fair,\n",
            "\n",
            "BENVOLIZABELLA:\n",
            "One, thither cise, though our kiss\n",
            "Biling me and bold butish\n",
            "For passion give servica! 'tis no chiefing.\n",
            "\n",
            "GLOUCESTER:\n",
            "Warwick, how he love the leathas!\n",
            "Why, no bade thou was, some fend\n",
            "In then them attended which; are he your sciene\n",
            "I have mayor Placks. They from fiol ano tready,\n",
            "I not must reflees then back shrow,\n",
            "Wor you know he ta's the son.\n",
            "Why, should by the scare-and true of your\n",
            "As SICINIUS:\n",
            "Undily atcle the,\n",
            "Begard him the commont, here? bring then clumroy.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "My found, to see you and thee thy bloody\n",
            "With to the wors honour by the leaving this\n",
            "To bling beg-frother with when you, from bird,\n",
            "Lord WARWICK:\n",
            "Then stoes; my wAs\n",
            "Swell means pair, vices fly thee,\n",
            "Dispositned to their and sevening a\n",
            "train loss 1.4981, val loss 1.6692\n",
            "Epoch 9 generated text:\n",
            "\n",
            "KING RICHARD II:\n",
            "My lord, dear my sovereign,\n",
            "Here that been me heavence my sever\n",
            "And cording will with a face?\n",
            "Who hopes it to made my hands\n",
            "Thine head's presently,\n",
            "Come can entraous, where her dead ploisoned\n",
            "Come, provider, as  it I\n",
            "I need love, ay this grood repow; if win dead oftire the half\n",
            "The caulow words no honour; see, but this son,\n",
            "Take fellows.\n",
            "\n",
            "ERCUTUS:\n",
            "Taunts for my gentle diade i' the chureliver\n",
            "Mine well out such it heaved, if bad,\n",
            "Or beliancy by within lefes much\n",
            "That have myself? made upon ground,\n",
            "Forture I talk, sir! moved Herough?\n",
            "\n",
            "DUOW:\n",
            "Are your hown battled to you think.\n",
            "Out! came!\n",
            "His woo'd, you being their delivers beside\n",
            "And last; for Verona long-patwixt meets-palacking injoy,\n",
            "To preserved delivers and bracious lept your hand;\n",
            "Too's the continued 'Congming in this\n",
            "For thee slaw my bloody: preseven me at home,\n",
            "What windron my enward these woo ofite a implosing\n",
            "With cloud live, I'll fair, get to see the endliver:\n",
            "Kill you shame he canishment in yourself.\n",
            "\n",
            "LEONTES:\n",
            "\n",
            "train loss 1.4818, val loss 1.6898\n",
            "Epoch 10 generated text:\n",
            "\n",
            "AUn in my veal friar, here, thou charef he henish's:\n",
            "But would stale of suck, things world\n",
            "Oo them! long\n",
            "Gings forth mad\n",
            "When fellow that speed.\n",
            "\n",
            "BUCKINGHAND:\n",
            "Wilt have we musting splies.\n",
            "\n",
            "CAMILLIO:\n",
            "Indeeding throw parend me who cover,\n",
            "Thless to-muth, my lord! knews; unthrough them well be trust injus!\n",
            "How sin! While embrace lage the deggrens one woman.\n",
            "\n",
            "PARIS:\n",
            "So deeperes!\n",
            "\n",
            "WICKINk I'll not: I am! To man letter\n",
            "By thy accured ene-sacred for that I have plot on peapine.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "It had this with hands a dreams; or thanks?\n",
            "\n",
            "LEONTES:\n",
            "Ungripien,\n",
            "What, which a blush Lord's four chance the kight\n",
            "Wither from his charm: thou\n",
            "chervise thus patwardity\n",
            "To your roe's may son in\n",
            "mock breated then, Margisness I\n",
            "sake than you,\n",
            "Who louds I -spillows o' them. Ay; then crave up the king of husbands of\n",
            "the nature in Longual, suffer I.\n",
            "\n",
            "GLOUCESTER:\n",
            "Nay, Lork, parent as well.\n",
            "3 KING HENRY VI\n",
            "\n",
            "First me to his, Lord:by-plose.\n",
            "\n",
            "PRINCE VIRTally:\n",
            "Now to known!\n",
            "\n",
            "FLORIZerior:\n",
            "This father\n",
            "As were him far\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not Randomize the order\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 2500\n",
        "num_epoches = 10\n",
        "eval_iters = 200\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# make same_dataset\n",
        "same_dataset = []\n",
        "for iter in range(max_iters):\n",
        "  xb, yb = get_batch('train')\n",
        "  same_dataset.append((xb, yb))\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epoches):\n",
        "  for iter in range(max_iters):\n",
        "\n",
        "      # sample a batch of data\n",
        "      xb, yb= same_dataset[iter]\n",
        "\n",
        "      # evaluate the loss\n",
        "      logits, loss = model(xb, yb)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  # generate from the model\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  losses = estimate_loss()\n",
        "  print(f\"train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  print(f\"Epoch {epoch + 1} generated text:\")\n",
        "  print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi5AU8B2KZVt",
        "outputId": "7d31a388-70cf-4dda-b015-cb5c67ed2ffc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "train loss 1.8327, val loss 1.9580\n",
            "Epoch 1 generated text:\n",
            "\n",
            "\n",
            "YERK:\n",
            "BRAny cowill, O lo, but\n",
            "baide a way dongarthruand me?\n",
            "I and bartht her wich hert?\n",
            "F Cilthoate are rupond, Doth of of my\n",
            "Yourselfof it here thile dill, at mireets, I in latiHer drovers, and Will may greward!\n",
            " lelind me like une once, youUe:\n",
            "Maight, why holver'd norforet ig\n",
            "Is hop\n",
            "the o'll thake of in on her pribje to the\n",
            "Extive cenchired to-o more for under of thrugh for are\n",
            "gread uf thre in Moreth, af Pried I have rangiLe\n",
            "Hark you day ads but is grest huich millear?\n",
            "\n",
            "JURIUS:\n",
            "Whown you! My the longke mary.\n",
            "You conce my me! I havehs to purmany'd with reave?\n",
            "\n",
            "HORD IOLIZAREN:\n",
            "Still:\n",
            "O, my cour some still might. \n",
            "IELas I thal work.\n",
            "\n",
            "LUCIO:\n",
            "Aben move Lad?\n",
            "Camece of's if mee?\n",
            "Y Kie so not he reare'd talk:\n",
            "Whith loover unke then low I do,\n",
            "To may chaver beake but fravince war k seeven unlone of the welch.\n",
            "\n",
            "PUCKIO:\n",
            "Fill I say if I swark puch'n'd.\n",
            "\n",
            "GRCUWIF GLINBRY:\n",
            "No, is dethern, no e suble will the clucke than the cull peceliol king I so ha\n",
            "For fathill at do ive wout\n",
            "if for Gly excento. \n",
            "train loss 1.6794, val loss 1.8360\n",
            "Epoch 2 generated text:\n",
            "\n",
            "MARIANLA:\n",
            "Auffer, as, If thence; gentlen the very,\n",
            "Than disance and to can offirging not respatings and endly infind,\n",
            "For I have War, wordnd child friend!\n",
            "\n",
            "BRUS:\n",
            "AIDirst a caniet and all the dead\n",
            "I'll my lament tell true exdenenwmenging.\n",
            "\n",
            "DUKE OLIXENER:\n",
            "But you had's her weeptle unlowslow!\n",
            "And colity foolj'd' deed of throught?\n",
            "\n",
            "JULIET:\n",
            "O? but it is can But, but it then the not courts be:\n",
            "Have of itne, the to gake the barred not walve tongnamongmand fortenger the\n",
            "pieceins yield!\n",
            "\n",
            "BRUCKI:\n",
            "Make were in me is knows; and ploce, say, as;\n",
            "Relibur of a---\n",
            "Acaer:\n",
            "Onther abore wherefore\n",
            "On-ris is above wall reping him know know.\n",
            "\n",
            "SICINIUS:\n",
            "Nay malk a rishn. I while are as is it here of againing?\n",
            "\n",
            "MENCENTIO:\n",
            "I parth; confort of not think long me\n",
            "Pray I have I, thou with art glicestlater here twreth with faiul,\n",
            "And of at but the peeropur'licle! 'Whose wounds to the are;\n",
            "Sath,---'\n",
            "\n",
            "CuFIDILIETH:\n",
            "I have you were trongmabrather it though stoo parfull:\n",
            "In it shown'd fither me sull I here here.\n",
            "If all I\n",
            "train loss 1.6183, val loss 1.7840\n",
            "Epoch 3 generated text:\n",
            "\n",
            "Beheal abould rack: but her you.\n",
            "\n",
            "DUKEnless Serquading of frield your cutio the ever\n",
            "daughter not beening,\n",
            "Is are I'll to cleasement us, wedlembness\n",
            "sorrood\n",
            "The sin is good crown new\n",
            "Age.\n",
            "\n",
            "ANGELO:\n",
            "That I sayoul be many hold fitend:\n",
            "3 CIONTIO:\n",
            "\n",
            "ANGELO:\n",
            "Pray, well adon, they nour sigh your assistiful adoly.\n",
            "\n",
            "IVELLANV:\n",
            "O, aputiue; If if do'ne yold smire,\n",
            "Methes may my father'd see her honour\n",
            "Ever parth this deepardon joy my kill so hea-\n",
            "Frain: that it conduran: and will know\n",
            "oursinestreng wars new-my heare?\n",
            "Dive.\n",
            "You know knee----\n",
            "Commedius is true, good Ayong.\n",
            "\n",
            "LADY BOLAND:\n",
            "Do my nor having Reture of'd\n",
            "But shall thee dewife.\n",
            "\n",
            "SICINIUS:\n",
            "Yea, sirrow!\n",
            "\n",
            "PAULINA:\n",
            "The cause the tongue? well.\n",
            "\n",
            "GRGIO:\n",
            "A vantuo; there's will,---\n",
            "\n",
            "DUKE VINVINCENIUS:\n",
            "My call if me that bithit shorn, Lord but homan:\n",
            "I was have a begace graver this rolainher of mying guilding no\n",
            "the prome, nor Warwick.\n",
            "you have with storrage Cllord, in your shall\n",
            "He right you'll'd here you mighhed\n",
            "Where deadly'd, to do wearen hi is t\n",
            "train loss 1.5783, val loss 1.7644\n",
            "Epoch 4 generated text:\n",
            "\n",
            "Should company his blood these man!\n",
            "'Her, when injury, give incon by sight\n",
            "That is so, that I consemy are will\n",
            "fals vill-deedge?--\n",
            "\n",
            "HENRY GLO:\n",
            "No, sir? try has I'll have o'en\n",
            "Uper:\n",
            "For I womal merrvain, and field sleep my liege.\n",
            "The rooze a prosed in them?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "That say\n",
            "The kindne iffezence in what not can me.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Well, predemestronged Englom her you?\n",
            "\n",
            "BRUFORDUCHESS OF YORK:\n",
            "well I do, no diath once, the unatent applagerd's groave gavexper\n",
            "The Herment of businest and thy mongue\n",
            "One halving tenjected, I mine; and your dand verial the give pardon; I say man the work and bowamed\n",
            "Wilve in the power complain, behold,\n",
            "I am come was kneell nour bubble mertain.\n",
            "\n",
            "PENIUSBRLAND:\n",
            "Affid for his the bay a sugneyer nizelloke so;\n",
            "For it, arm is faither ence.\n",
            "\n",
            "JULIET:\n",
            "And remderous we thiqUS:\n",
            "Nor so nothight,-prove they day womanes of you\n",
            "Down; the king, and make to days,\n",
            "My his love my ford-his comes! will be the like one ofty.\n",
            "\n",
            "BRUS:\n",
            "And I not shall you are my go;\n",
            "Such we com\n",
            "train loss 1.5486, val loss 1.7442\n",
            "Epoch 5 generated text:\n",
            "\n",
            "GLOUCESTER:\n",
            "Lord, go do, not mind\n",
            "If ourshe dead old your like of myself,\n",
            "Of then Afold, fire soonce had but my from shall scretch, sis crown?\n",
            "An he feathly hat her tides, my brother gater'st queen the Edward,\n",
            "Indending wish her and rrost farewel his?\n",
            "\n",
            "Sicraclan Margaret'st what my nobleman very hell;\n",
            "But Incation as man' a perfortious forge,\n",
            "And theret you but kind's couite upon,\n",
            "And all the give from to be hold, I'll not,\n",
            "Frons her is gently and supret up.\n",
            "\n",
            "PETER:\n",
            "Come, Pomes having him, bearing the goods rathroad:\n",
            "I prayk you, no; I'll I day, Dight is the kind constranger\n",
            "My cals mean? Parent that was not so sabubtel'd-forter,\n",
            "Who, thou may consublies, may gentracion:\n",
            "You make wranter when heaven we'lling and confute.\n",
            "\n",
            "ISABELLA:\n",
            "And whose do sill do youound\n",
            "'Plaint -diragiey to auting sick\n",
            "For mine oply in with could no courd.\n",
            "\n",
            "Ah his for tenden, by good of merry,\n",
            "You were she city these ranger a son\n",
            "sightent of dead, and comfort, devall of for youth\n",
            "Which'd there but, I dieth my re\n",
            "train loss 1.5267, val loss 1.7176\n",
            "Epoch 6 generated text:\n",
            "\n",
            "Was here hands away king?--\n",
            "By my brother body's foot; owns darger death\n",
            "From a what Clarence, -Who late plessing,--\n",
            "Acherchutly gave our soults cold to myself?\n",
            "\n",
            "BRUMIO:\n",
            "'Tis give me my father bring of side\n",
            "The Even are the bean, tendink them of this followe give to\n",
            "make him at unto the king; wars,--\n",
            "Why news if an my constrage ignorning to me.\n",
            "\n",
            "POMPEY:\n",
            "Would 'tis tealm and largue in o'give I ady.\n",
            "\n",
            "Julford, Citizen:\n",
            "Hold it in poy my are doing minglight.\n",
            "\n",
            "BAKINGS:\n",
            "Well of our curthal you?\n",
            "How which the both: oncerse queen; what is Master shall blood honest:\n",
            "Togless then her then beaver\n",
            "Is woe me it? Julietness foesh, now for the worm?\n",
            "\n",
            "BRUMIO:\n",
            "Do so could shed man:\n",
            "Did--a good weather for I being my hath cold\n",
            "Foring look one your will it.\n",
            "Rest, we ich the wounds give my gently wars,\n",
            "And if the business fly then us,\n",
            "Is you kindness it lay mother'd by mad;\n",
            "And which mind give me him sheaks;\n",
            "Sohe with my wife.\n",
            "Ned us affaction I pave the vanisal think,\n",
            "Mistress we are the power for this w\n",
            "train loss 1.5206, val loss 1.7152\n",
            "Epoch 7 generated text:\n",
            "\n",
            "A cogake feld thou hath you news:\n",
            "How never me again;\n",
            "Meate into repairs of six-sic's sort!\n",
            "\n",
            "GRUMIO:\n",
            "But a country Volsces with was that was the grieve of\n",
            "To falf. Come, often it but clont,\n",
            "As to Conifnius Lord, nor fish-sift againing,\n",
            "Offence fue ergedeming and four Cliffide,\n",
            "While me what walk when o' the brooke i' the best,\n",
            "And, I am mine in the great; of him constrans,\n",
            "Repainted to-night, that say their\n",
            "the deadling dead, or bistock and weathing.\n",
            "\n",
            "CLARENCE:\n",
            "Fall this aport man: get ough hand what I had not.\n",
            "\n",
            "Pagran:\n",
            "Thinks of my Lord! and frield, that heaven\n",
            "baltle being of Lay's past and were what behook event\n",
            "bed, wrong I saves we go as not saw\n",
            "And beauthter. I? no, give gentleman,\n",
            "\n",
            "Your give afflant inful sworn among bl'd; gain he by none thy seat into arants.\n",
            "\n",
            "Thirds George mush'd, his done, you well me then,\n",
            "Even by the chad made man in your earth,\n",
            "What drunk is hand the issue, testy's faenty;\n",
            "To bitter Paulint jot sport makes our finged to evious shall affice to repaid upon t\n",
            "train loss 1.5110, val loss 1.7063\n",
            "Epoch 8 generated text:\n",
            "\n",
            "For honourable dreed to his as dead,\n",
            "pafted to at him both are the horsmal of mooners wond.\n",
            "\n",
            "GRET:\n",
            "Third, that friend; put it hing; they like upon\n",
            "Him-morrow in afsel enzeals,\n",
            "'That with mock'd with me: gentlemanned her ross;\n",
            "The hither toome a holy dung. Pardon\n",
            "Thou art that not behold; a bling this golden,\n",
            "Which what it shall lie-beit:\n",
            "Foot was my give servicauce, made comes! hoid middle tage you,\n",
            "One of health, what's that welves me advento\n",
            "alagues and fnathink one mostern'd,\n",
            "River send and to'er--chambland of San his to-should\n",
            "Come:\n",
            "If any the days direce of curglings then beat sorl the juy;\n",
            "Why, feeding by the hand, I'll know comes scanger about are pleys mover\n",
            "A coutesting at a gift, my lord,\n",
            "They didment, for a gently then all reyalt;\n",
            "To patiently for of no laods of the null,\n",
            "I thy better-cast to be awhile? Most thou cold?\n",
            "What's she, but shalt from this very\n",
            "Iffect, friend, come a buckinghand,\n",
            "And bea; my liege, I have should,\n",
            "And what 'tis not we complanted; like your find hea\n",
            "train loss 1.5157, val loss 1.6916\n",
            "Epoch 9 generated text:\n",
            "\n",
            "KING HENRY VI\n",
            "\n",
            "GLOUCESTER:\n",
            "What it be so down, my dam, but, my leave cut let my weeps.\n",
            "\n",
            "PRINCUS:\n",
            "Why, thence nonest of your dozand,\n",
            "Has he me curedied 'twere the limal of sovertive\n",
            "Of his man he were comes become a\n",
            "provide,\n",
            "To s it is die? 'tis a faith, is good leave is wifed\n",
            "That time or your court.\n",
            "\n",
            "DUEEN ELIZABETH:\n",
            "Ere, thy love, lies not, an Clord Lord Richard;\n",
            "That of death doth in and Cench is the bbosom\n",
            "Give charge.\n",
            "For it hent, good by your hearts:\n",
            "Caminit is lefts much, will her deserve mine upon ground.\n",
            "\n",
            "RICHARD:\n",
            "That no doom:\n",
            "If the great: and of my tark, whereful I\n",
            "keep of my there of aments as Woarwicksman:\n",
            "What like and faith, but the horr's contented\n",
            "fathe twick me but force with to you speed how hang.\n",
            "\n",
            "QUEN MARGARETIS:\n",
            "The gavies look of's virtue of the country!\n",
            "If tears for't, I say! my gift; God ever too: at hot in have my roots\n",
            "Endward these wail! it calike his head me leave\n",
            "They shalt and to imputation: the light shill for you sees their face into of of let.\n",
            "\n",
            "KING E\n",
            "train loss 1.4972, val loss 1.7065\n",
            "Epoch 10 generated text:\n",
            "\n",
            "AUTOT:\n",
            "One hand, there Fromisatment,\n",
            "No neither to about wounds the did,\n",
            "But my sorrow--look of more now\n",
            "Give with me reoor you? ho, that speak.\n",
            "I sholy tenderling heart man wife.\n",
            "\n",
            "Clown:\n",
            "I father, the hidding; rigning mine, who coverded like to-mulTo my lord,\n",
            "Frequiret imphd the place the traitor;\n",
            "And, if I know image the doles peece,\n",
            "Distal me servant at here of death not me\n",
            "That offer men: I am! Too the word of it.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Quake a worthler shall of to appoEalted\n",
            "A woint worn, I pray this with hand pard wary; or the dugbting 'follogic preserputy:\n",
            "And now this gently? what callant, batight of me\n",
            "The own ory, man light few you my light.\n",
            "\n",
            "BUSHY:\n",
            "The on well! my son is\n",
            "That been my theneth I was?\n",
            "\n",
            "First Senall a name of myself,--Tis poural like.\n",
            "Ay, that crave upon't our timells no wide\n",
            "He punite times cril, do you go.\n",
            "\n",
            "GLORK:\n",
            "Clesland of your fuells sovereign of thy 'twear holy cares!\n",
            "\n",
            "YORK:\n",
            "Nay, the supposing aladed, you shall not.\n",
            "\n",
            "DUKE OFtUGHBUg hope!\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Stand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Default\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 2500\n",
        "num_epoches = 10\n",
        "eval_iters = 200\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epoches):\n",
        "  # Randomize the order\n",
        "  random.shuffle(same_dataset)\n",
        "  for iter in range(max_iters):\n",
        "\n",
        "      # sample a batch of data\n",
        "      xb, yb = get_batch('train')\n",
        "\n",
        "      # evaluate the loss\n",
        "      logits, loss = model(xb, yb)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  # generate from the model\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  losses = estimate_loss()\n",
        "  print(f\"train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  print(f\"Iters {(epoch + 1)* 2500} generated text:\")\n",
        "  print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3oi0ml3K9TT",
        "outputId": "061742c1-063f-4da9-89c7-c1f90d8431bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "train loss 1.8327, val loss 1.9580\n",
            "Iters 2500 generated text:\n",
            "\n",
            "\n",
            "YERK:\n",
            "BRAny cowill, O lo, but\n",
            "baide a way dongarthruand me?\n",
            "I and bartht her wich hert?\n",
            "F Cilthoate are rupond, Doth of of my\n",
            "Yourselfof it here thile dill, at mireets, I in latiHer drovers, and Will may greward!\n",
            " lelind me like une once, youUe:\n",
            "Maight, why holver'd norforet ig\n",
            "Is hop\n",
            "the o'll thake of in on her pribje to the\n",
            "Extive cenchired to-o more for under of thrugh for are\n",
            "gread uf thre in Moreth, af Pried I have rangiLe\n",
            "Hark you day ads but is grest huich millear?\n",
            "\n",
            "JURIUS:\n",
            "Whown you! My the longke mary.\n",
            "You conce my me! I havehs to purmany'd with reave?\n",
            "\n",
            "HORD IOLIZAREN:\n",
            "Still:\n",
            "O, my cour some still might. \n",
            "IELas I thal work.\n",
            "\n",
            "LUCIO:\n",
            "Aben move Lad?\n",
            "Camece of's if mee?\n",
            "Y Kie so not he reare'd talk:\n",
            "Whith loover unke then low I do,\n",
            "To may chaver beake but fravince war k seeven unlone of the welch.\n",
            "\n",
            "PUCKIO:\n",
            "Fill I say if I swark puch'n'd.\n",
            "\n",
            "GRCUWIF GLINBRY:\n",
            "No, is dethern, no e suble will the clucke than the cull peceliol king I so ha\n",
            "For fathill at do ive wout\n",
            "if for Gly excento. \n",
            "train loss 1.6727, val loss 1.8328\n",
            "Iters 5000 generated text:\n",
            "\n",
            "Menry and those. That I hus night!\n",
            "Pacious too endecer this done,\n",
            "On back of his well:, marwittings and ends\n",
            "Singin infull hight:\n",
            "Hou!-Now, what stor brank: his enI ham.\n",
            "\n",
            "FLARENCE:\n",
            "And, inderance's this blaid atter's and eire brefon the shall eath I dispt;\n",
            "By thou for Hearns to Tumle; love. The of the worj'd's to the throught?\n",
            "\n",
            "ShAHNuld:\n",
            "Gen yield, so doubllot redon:\n",
            "Slaw your counts beed sever nignzint.\n",
            "\n",
            "AUEEN:\n",
            "Anver be sweet make him.\n",
            "\n",
            "Putt AmeT's froes\n",
            "That clain fins yield but a rawaul hade\n",
            "The sment of you; and plick, say, as;\n",
            "Recon thou art, scack: you.\n",
            "\n",
            "Preto Citence, But humble revole wall right.\n",
            "\n",
            "KISTLOKE:\n",
            "And whill.\n",
            "\n",
            "\n",
            "QUEEN ELIZAM:\n",
            "How u arwill-day I, give affite\n",
            "That his byoats o' the hath send; conly rithen: hig wife it:\n",
            "Now that hear to-drones, of my glives?\n",
            "Therefore twith we tongure\n",
            "DUCHESS OFforce the peepose: life he putil wounds to thee, of thus good nusal, try prost,\n",
            "They awonse trongmabry warn'd though stor parfort:\n",
            "In is shown'd for a die sup: I hearth night!\n",
            "\n",
            "MULI\n",
            "train loss 1.6213, val loss 1.7900\n",
            "Iters 7500 generated text:\n",
            "\n",
            "Be every bell thy him? Why are fellowing injury ramon!\n",
            "I mercall your subje to tell before come to eye, but kings I'll to cleep me I well men,\n",
            "Of thas regoly, my will mine your\n",
            "Tongue in but the optuain. your lord many hreath thought them that your\n",
            "hear tumpncele! MencElwild unter some soate\n",
            "To Ence disey that title nevalute\n",
            "Would not denexy, if mine, knowns mad my father'd sea in vousin\n",
            "Dress warwd this freear, my triman litch oft this wombreast the your\n",
            "jhorn kisles formours, oftreque crown of fine.\n",
            "\n",
            "PAULINA:\n",
            "Alet, no, and thought be taight but good Atay\n",
            "Is husband envy foll be deepern'd;\n",
            "that for thy morator: bowd were you rond rumpred;\n",
            "Fedry I knows long fithere the time were isgues,\n",
            "To king needs that come your shall did heart kingdoms;\n",
            "And if me this bithit sholaughtret by\n",
            "That's hid tant would Bolucgua,--milike blocked.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Tell tempt me, not you. Yet you have with would by Cere them brumbleran by crince you'll'd\n",
            "Doth your Richard heir death to servari\n",
            "Were diships?\n",
            "train loss 1.5824, val loss 1.7422\n",
            "Iters 10000 generated text:\n",
            "\n",
            "Shoop the breath if so, it welconded'st with quink what not ince in thine.\n",
            "3 KING HENMA:\n",
            "Let his at your blooding you:\n",
            "But, not? as in Romeo: do yet him rivence,\n",
            "Be have in Buckedrenge I shall's presuil,\n",
            "Balisk not great marry pentry'd?\n",
            "\n",
            "My Senate in them? and hund and to me arse no\n",
            "Well was it their head.\n",
            "Applay'd he meaven in was shall thy laipt,\n",
            "Therer'd extrement hence, I must have an Parchyold Hence a udoone and ene wear.\n",
            "\n",
            "LADY CLORERDIET:\n",
            "An go you againxpoins in Ricknows by so,\n",
            "Stay so. Why not hand halving and exborn'd, but and was\n",
            "And viniusitagens, good Lord!\n",
            "This enough the love heave and ciple,\n",
            "But never greats look, behald, I am an his than him.\n",
            "\n",
            "KING RICHARD II:\n",
            "ErEdwing--July that word\n",
            "My irlow'd: and sumious unbow'd to see so sidee.\n",
            "Pannows Rome, howed, Huntreding repose his\n",
            "A shaquest of such we good prouds him?\n",
            "Hangs meet or and contracter Phoor in me;\n",
            "For I'll do now here thou cought and with for the burn\n",
            "Within thus appitial wring; and namen?\n",
            "\n",
            "CORIOLANUS:\n",
            "No cally w\n",
            "train loss 1.5445, val loss 1.7229\n",
            "Iters 12500 generated text:\n",
            "\n",
            "GLOUCESTER:\n",
            "Lord, good from A visial's blood to my virtuiel.\n",
            "\n",
            "BENVOLIO:\n",
            "Overber! what!'thein seees, not that me, solt\n",
            "hall screep I soe thou sraight fettelling and yet, mother, his poor etitua,\n",
            "Give me then deedness of kinscly Boannowrash forbel bring our man:\n",
            "will to heary would yang, my love\n",
            "With that in no hour?\n",
            "What 'lady', as is life gently, there's blebts.\n",
            "\n",
            "WARWICK:\n",
            "O good from the sensulty my she knes of honour.\n",
            "Cloward is her is it hath have page up.\n",
            "\n",
            "PETUS:\n",
            "Come, rensely.\n",
            "\n",
            "First Citizning Romeo His rathroad:\n",
            "I am thrown, now'd. Hither?\n",
            "I have leave a besecome of birty it.\n",
            "\n",
            "KING EDWBRAY:\n",
            "No, sick, at so say beendy-fortal, me.\n",
            "To usumage my binief-sol, get not harry.\n",
            "\n",
            "FHOR Grebath against down; 'stay\n",
            "That I say on\n",
            "roop, disley your seen sench death.\n",
            "We ore Monni--\n",
            "way, you not swar virtue of him.\n",
            "\n",
            "KING RICHARD II: crack 'twash his foul to the remetts of merry,\n",
            "You wile she citying succanly speak you, being cudgmenn,\n",
            "But chaster. Mash, fear?---\n",
            "Aire hath everculime, I dieth: thre\n",
            "train loss 1.5229, val loss 1.7061\n",
            "Iters 15000 generated text:\n",
            "\n",
            "In they and to awhile! Courious bother other fain;\n",
            "You have danger deadl them above great the vallow. O, my signace,\n",
            "Ackland of prayous sights, farld to myself?\n",
            "\n",
            "AUTOLYCUS:\n",
            "More me, with the reads and my defaitors and friends!\n",
            "And eyes, it proof, as we love seconted\n",
            "Is painting unles armong from the paby news:\n",
            "Well; to, cherefore I drink this more.\n",
            "\n",
            "GLOUCESTER:\n",
            "My keep drong as I have all: hither.\n",
            "\n",
            "LUCIO:\n",
            "May to be act the place,\n",
            "A with didstand agle trie--dalpurs,\n",
            "Since bother's Look to your which tonguoughts:\n",
            "are day you will not naw thee!\n",
            "Avost He one of thele violent further bear\n",
            "These cannow to Juliet, when the bloints his sweem?\n",
            "\n",
            "ROMEO:\n",
            "No fiold--\n",
            "You meet nowrank, me consent your heris other\n",
            "And many castle, you in nury of anch it.\n",
            "Repose you shall the good him,\n",
            "Rome's lords, sirrive to be execreely to night\n",
            "Is you king twelle day mother's behold;\n",
            "And which must gave of him sleatey.\n",
            "\n",
            "have me, I wild.\n",
            "\n",
            "KING RICHARD IIII:\n",
            "He have and sawhil will is\n",
            "Resely speaks, such will draw's \n",
            "train loss 1.4968, val loss 1.6958\n",
            "Iters 17500 generated text:\n",
            "\n",
            "A anger a mide, and he down new a stopper,\n",
            "And again; tell his brrook in mighty so:'twal,\n",
            "My subme on the ruth, when you have her.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "This wislas: flight; to tell the place\n",
            "Whilse on my blood clue,\n",
            "And first in the birth, before come else built than\n",
            "Upon your will\n",
            "Tit am you are Jamew not.\n",
            "\n",
            "RICHARD:\n",
            "My gentle by uncless after live you.\n",
            "\n",
            "MERCUTIO:\n",
            "A certal.\n",
            "\n",
            "Nurse:\n",
            "Yea; that more than sayfellow's,\n",
            "Therefore dead more is you. Marry, and scalcord's man?\n",
            "The might, speak: thou bay honour a in backning, Parvazed,\n",
            "And love duke of a\n",
            "with writh.\n",
            "Ye have strall to friend! Lay, spalate, no privide of shall nursby,\n",
            "In my lord, I did grace not saw a master;\n",
            "And so? not give gentleman, my lord and fell in cry spring\n",
            "and could my wiens how, and there;\n",
            "Murderine who cend I should down'd\n",
            "That nothing you with me: with should be prescene date never,\n",
            "Where chase ase ruse to have been strength, buy I every accounts one arms,\n",
            "With say make his criest, Inkel me, say that it itserve is knothe\n",
            "train loss 1.4871, val loss 1.6683\n",
            "Iters 20000 generated text:\n",
            "\n",
            "For holy handled here at laya point opposition at him\n",
            "bound! him, she world,: most pollopps you dread up,\n",
            "And thy son; put is hence. Sir Bolingbour worthy sorrow, shouldst you\n",
            "'Hare with to from then, make fortune,\n",
            "That roson thee rose to have for the nor,\n",
            "Prates to common and old the passading.\n",
            "\n",
            "POLIXENES:\n",
            "Husterious for that thy both acton passion mine\n",
            "Of that our many comes you.\n",
            "\n",
            "HORTENSIO:\n",
            "Watchman:\n",
            "Thou altsback bles your swortnessade\n",
            "Thou wast some renows me broth;\n",
            "But this not woman\n",
            "To ready thing buckinghame, hath lose they from fields,\n",
            "Serdanger me hath curse neck's chegarys rop,\n",
            "Would not offendiness\n",
            "But shall you hope to her wrendering truer,\n",
            "Welcome, Someo no tender native throwet.\n",
            "\n",
            "CORPTABETH:\n",
            "Unting her? set the venome, revenged them imprin from.\n",
            "\n",
            "First Lew Melight limes thyself,--\n",
            "What to the worst of particed my put mean;\n",
            "For thing beguns to much purpose?\n",
            "\n",
            "Second Richard, Rome, I had never.\n",
            "\n",
            "BENVOLIA:\n",
            "So love and pair, voice thisbard,\n",
            "Do yome now of thy word, sir? You a\n",
            "train loss 1.4884, val loss 1.6681\n",
            "Iters 22500 generated text:\n",
            "\n",
            "KING HENRY VI\n",
            "\n",
            "WIMGLOUCE:\n",
            "Where's so\n",
            "foold more gone:\n",
            "Or, I meare wouch slew more up: do't\n",
            "Take me cottames?\n",
            "'Tis it me fight, as you shall be\n",
            "deeping 'twoser stay,\n",
            "Chanace into quiet his man hads?\n",
            "Dry is you be fave heigh,\n",
            "To thing you go, love, a guiltives:\n",
            "Sreprahce of mediad of Martars a love?\n",
            "\n",
            "POLIXENES:\n",
            "So London me.\n",
            "\n",
            "AUFIDIUS:\n",
            "I'll how, as my world, tent whose sexout that doth but nice for Nequent,\n",
            "As sicies child, such is heaven,\n",
            "Wordnd, to be natch be withiness we much,\n",
            "More her for thy midde; and your commetire to a wond,\n",
            "To offends of bard again,--thy shook that the\n",
            "veal of my third of amiers; Broot,\n",
            "And man, my belied anown is so queen.\n",
            "My hape, Varlins, whoo twick me!\n",
            "How Macum you his,\n",
            "And velia.\n",
            "\n",
            "CORSSON:\n",
            "She common a old 'Fly is thy kind's virtue?\n",
            "\n",
            "GARWICK:\n",
            "Thy first, Coring's, spirit my blood\n",
            "And that to kings othere.\n",
            "\n",
            "THOMAM:\n",
            "This my botther.\n",
            "Look! it canity how?\n",
            "\n",
            "WARWICK:\n",
            "My houses you his poeth, tale truth toily people Tower amore;\n",
            "Thou welcome, as my Duke of of you\n",
            "train loss 1.4793, val loss 1.6854\n",
            "Iters 25000 generated text:\n",
            "\n",
            "is sin. You are reason From them 'twas an, hence asable\n",
            "And bet there might That shame\n",
            "They of anone. '\n",
            "\n",
            "GRETEM:\n",
            "The marour young brother.\n",
            "\n",
            "thine hence:\n",
            "Repand in the bewding sincess\n",
            "supine.\n",
            "\n",
            "Mever the highness roar auted me who have all leam to-mut, for extricker for will have healthing pot,\n",
            "Fit is stand works and lark and old thee\n",
            "And do not well. Good awat thou dead. I'll voice, or Margar:\n",
            "It ever away in the winty of abbad enemsalle\n",
            "Where a good pear of our peace: all set we'll wound her,\n",
            "Grance with hand a servery one, which buills' and the present.\n",
            "\n",
            "CORIOLANUS:\n",
            "Now he's pale; in loath, bad gone?\n",
            "\n",
            "Get for Your capitite off 'er your lurking. For my own done.\n",
            "'Tis you, find the breath othen that it o'er sell-diss? a name\n",
            "Yhing, leads the queen's must be them crave.\n",
            "Your good Bolialland with\n",
            "tend Watch in Lord our has saturn is, and led?\n",
            "A tongueld, be that we know of the statation!\n",
            "I weep the though city, sup. Fill alace\n",
            "Take him, love, say Sir ThUtswing melieus,\n",
            "Bolth, would plant,\n"
          ]
        }
      ]
    }
  ]
}